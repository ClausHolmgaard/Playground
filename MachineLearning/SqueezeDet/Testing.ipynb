{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initially just some playing round with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Image<br>\n",
    "Initial output: center of hand<br>\n",
    "Is anchors needed? So the prediction is an offset?<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SqueezeDetHelpers import fire_layer, binary_crossentropy, keras_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid over image size\n",
    "    - Grid nodes will be anchors\n",
    "    - Net predicts: Probability of class at anchor, and offset from anchor.\n",
    "        - In later versions, several offsets will be predicted at each offset.\n",
    "- The net is fully convolutional, meaning the output must be feature maps.\n",
    "    - Amount of output filters will then be confidence+x_offset+y_offset\n",
    "    - filter size will be the size of the anchor grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_out = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/annot\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"./data\"\n",
    "ANNOTATION_FILE = r\"annot\"\n",
    "annotation = os.path.join(DATA_DIR, ANNOTATION_FILE)\n",
    "print(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 320\n",
    "WIDTH = 320\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 0.001\n",
    "KEEP_PROB = 0.5\n",
    "CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_HEIGHT = 20\n",
    "ANCHOR_WIDTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_WEIGHT = 1.0\n",
    "OFFSET_LOSS_WEIGHT = 1.0\n",
    "OFFSET_WEIGHT = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out dim: 20x20\n",
      "Number of anchor nodes: 400\n"
     ]
    }
   ],
   "source": [
    "num_anchor_nodes = ANCHOR_HEIGHT * ANCHOR_WIDTH\n",
    "\n",
    "print(f\"Out dim: {ANCHOR_HEIGHT}x{ANCHOR_WIDTH}\")\n",
    "print(f\"Number of anchor nodes: {num_anchor_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anchors: 400\n",
      "Anchor dimension: (20, 20)\n",
      "Anchor shape: (20, 20, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([45, 45], dtype=uint32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_anchors():\n",
    "    \n",
    "    #anchors = np.zeros((num_anchor_nodes, 2))\n",
    "    anchors = np.zeros((ANCHOR_HEIGHT, ANCHOR_WIDTH, 2), dtype=np.uint32)\n",
    "    print(f\"Number of anchors: {num_anchor_nodes}\")\n",
    "    \n",
    "    print(f\"Anchor dimension: ({ANCHOR_HEIGHT}, {ANCHOR_WIDTH})\")\n",
    "    print(f\"Anchor shape: {anchors.shape}\")\n",
    "    \n",
    "    #xs = np.arange(PIXELS_BETWEEN_ANCHORS, WIDTH, PIXELS_BETWEEN_ANCHORS)\n",
    "    #ys = np.arange(PIXELS_BETWEEN_ANCHORS, HEIGHT, PIXELS_BETWEEN_ANCHORS)\n",
    "    \n",
    "    x_start = WIDTH / (ANCHOR_WIDTH + 1)\n",
    "    x_end = WIDTH - x_start\n",
    "    y_start = HEIGHT / (ANCHOR_HEIGHT + 1)\n",
    "    y_end = HEIGHT - y_start\n",
    "    xs = np.linspace(x_start, x_end, num=ANCHOR_WIDTH, dtype=np.uint32)\n",
    "    ys = np.linspace(y_start, y_end, num=ANCHOR_HEIGHT, dtype=np.uint32)\n",
    "    \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    for cx in range(len(xs)):\n",
    "        for cy in range(len(ys)):\n",
    "            anchors[counter] = [xs[cx], ys[cy]]\n",
    "            counter += 1\n",
    "    \"\"\"\n",
    "    \n",
    "    for ix in range(ANCHOR_HEIGHT):\n",
    "        for iy in range(ANCHOR_WIDTH):\n",
    "            anchors[ix, iy] = (xs[ix], ys[iy])\n",
    "    \n",
    "    return anchors\n",
    "    \n",
    "anchs = set_anchors()\n",
    "anchs[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (?, 320, 320, 3)\n",
      "conv1: (?, 160, 160, 64)\n",
      "pool1: (?, 80, 80, 64)\n",
      "fire1: (?, 80, 80, 256)\n",
      "fire2: (?, 80, 80, 256)\n",
      "pool2: (?, 40, 40, 256)\n",
      "fire3: (?, 40, 40, 384)\n",
      "fire4: (?, 40, 40, 384)\n",
      "pool3: (?, 20, 20, 384)\n",
      "pred_conf: (?, 20, 20, 1)\n",
      "pred_offset: (?, 20, 20, 2)\n",
      "preds: (?, 20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(HEIGHT, WIDTH, CHANNELS), name=\"input\")\n",
    "print(f\"input: {input_layer.shape}\")\n",
    "\n",
    "conv1 = Conv2D(name='conv1', filters=64, kernel_size=(3, 3), strides=(2, 2), activation='relu', padding=\"SAME\",\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.01),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(input_layer)\n",
    "print(f\"conv1: {conv1.shape}\")\n",
    "\n",
    "\n",
    "pool1 = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='SAME', name=\"pool1\")(conv1)\n",
    "print(f\"pool1: {pool1.shape}\")\n",
    "\n",
    "fire1 = fire_layer(name=\"fire1\", input=pool1, s1x1=32, e1x1=128, e3x3=128, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire1: {fire1.shape}\")\n",
    "\n",
    "fire2 = fire_layer(name=\"fire2\", input=fire1, s1x1=32, e1x1=128, e3x3=128, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire2: {fire2.shape}\")\n",
    "\n",
    "pool2 = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='SAME', name=\"pool2\")(fire1)\n",
    "print(f\"pool2: {pool2.shape}\")\n",
    "\n",
    "fire3 = fire_layer(name=\"fire3\", input=pool2, s1x1=48, e1x1=192, e3x3=192, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire3: {fire3.shape}\")\n",
    "\n",
    "fire4 = fire_layer(name=\"fire4\", input=fire3, s1x1=48, e1x1=192, e3x3=192, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire4: {fire4.shape}\")\n",
    "\n",
    "pool3 = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='SAME', name=\"pool3\")(fire4)\n",
    "print(f\"pool3: {pool3.shape}\")\n",
    "\n",
    "\"\"\"\n",
    "fire4 = fire_layer(name=\"fire4\", input=pool2, s1x1=16, e1x1=64, e3x3=64, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire4: {fire4.shape}\")\n",
    "\n",
    "fire5 = fire_layer(name=\"fire5\", input=fire4, s1x1=16, e1x1=64, e3x3=64, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire5: {fire5.shape}\")\n",
    "\n",
    "fire6 = fire_layer(name=\"fire6\", input=fire5, s1x1=16, e1x1=64, e3x3=64, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire6: {fire6.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "conv2 = Conv2D(name='conv2', filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"SAME\",\n",
    "               #filters=len(ANCHORS),\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(pool2)\n",
    "print(f\"conv2: {conv2.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "conv3 = Conv2D(name='conv3', filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"SAME\",\n",
    "               #filters=len(ANCHORS),\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(conv2)\n",
    "print(f\"conv3: {conv3.shape}\")\n",
    "\n",
    "conv4 = Conv2D(name='conv4', filters=256, kernel_size=(3, 3), strides=(2, 2), activation='relu', padding=\"SAME\",\n",
    "               #filters=len(ANCHORS),\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(conv3)\n",
    "print(f\"conv4: {conv4.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "preds = Conv2D(name='preds', filters=num_out, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid', padding=\"SAME\",\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY)\n",
    "               )(fire6)\n",
    "print(f\"preds: {preds.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "#drop1 = Dropout(rate=KEEP_PROB, name=\"drop1\")(fire6)\n",
    "\n",
    "pred_conf = Conv2D(name='pred_conf', filters=1, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid', padding=\"SAME\",\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.01),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY)\n",
    "               )(pool3)\n",
    "print(f\"pred_conf: {pred_conf.shape}\")\n",
    "\n",
    "pred_offset = Conv2D(name='pred_offset', filters=2, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid', padding=\"SAME\",\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.01),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY)\n",
    "               )(pool3)\n",
    "print(f\"pred_offset: {pred_offset.shape}\")\n",
    "\n",
    "#preds = Concatenate()([pred_conf, pred_offset])\n",
    "preds = concatenate([pred_conf, pred_offset])\n",
    "print(f\"preds: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-entropy: q * -log(p) + (1-q) * -log(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    # We are predicting a batchsize x anchorwidth x anchorheight x 3 output.\n",
    "    c_predictions = y_pred[:, :, :, 0]\n",
    "    c_labels = y_true[:, :, :, 0]\n",
    "    \n",
    "    pred_offset_x = 2 * (y_pred[:, :, :, 1] - 0.5) * OFFSET_WEIGHT\n",
    "    pred_offset_y = 2 * (y_pred[:, :, :, 2] - 0.5) * OFFSET_WEIGHT\n",
    "    \n",
    "    true_offset_x = y_true[:, :, :, 1]\n",
    "    true_offset_y = y_true[:, :, :, 2]\n",
    "    \n",
    "    #offset_mask = K.copy(true_offset[:,:,:,0])\n",
    "    #offset_mask = K.abs(true_offset[:,:,:,0]) + K.abs(true_offset[:,:,:,0])\n",
    "    \n",
    "    #offset_mask[offset_mask!=0] = 1.0\n",
    "    \n",
    "    #offset_mask = K.zeros((BATCHSIZE, ANCHOR_WIDTH, ANCHOR_HEIGHT))\n",
    "    #m = K.where((true_offset[:,:,:,0]!=0) | (true_offset[:,:,:,1]!=0))\n",
    "    #offset_mask[true_offset[:,:,:,0]!=0] = 1.0\n",
    "    #offset_mask[true_offset[:,:,:,1]!=0] = 1.0\n",
    "    \n",
    "    g_x = K.less(true_offset_x, 0)\n",
    "    l_x = K.greater(true_offset_x, 0)\n",
    "    g_y = K.greater(true_offset_y, 0)\n",
    "    l_y = K.less(true_offset_y, 0)\n",
    "    \n",
    "    g_x_i = K.cast(g_x, dtype='float32')\n",
    "    l_x_i = K.cast(l_x, dtype='float32')\n",
    "    g_y_i = K.cast(g_y, dtype='float32')\n",
    "    l_y_i = K.cast(l_y, dtype='float32')\n",
    "\n",
    "    mask_offset_x = K.clip(g_x_i + l_x_i, 0, 1.0)\n",
    "    mask_offset_y = K.clip(g_y_i + l_y_i, 0, 1.0)\n",
    "    #zeroes = K.zeros_like(true_offset_x)\n",
    "    #ones = K.ones_like(true_offset_x)\n",
    "    #mask_offset = K.where(mask_offset_bool, ones, zeroes)\n",
    "    \n",
    "    #pred_conf = K.sigmoid(c_predictions)\n",
    "    #pred_conf = c_predictions\n",
    "\n",
    "    #c_loss = K.sum(\n",
    "    #    -(c_labels * K.log(pred_conf + EPSILON) + (1-c_labels) * K.log(1-pred_conf + EPSILON))\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    c_labels * (-K.log(c_predictions + EPSILON)) + (1-c_labels) * (-K.log(1-c_predictions + EPSILON)) * c_labels\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.maximum(K.abs(c_predictions), 0) - c_predictions * c_labels + K.log(1 + K.exp(-K.abs(c_predictions)))\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.maximum(K.abs(c_predictions), 0) - c_predictions * c_labels + K.log(1 + K.exp(-K.abs(c_predictions)))\n",
    "    #, axis=0) / BATCHSIZE\n",
    "\n",
    "    #c_loss = K.sum(c_loss) / (ANCHOR_HEIGHT * ANCHOR_WIDTH)\n",
    "    \n",
    "    #diff = K.abs(c_labels - c_predictions)\n",
    "    #c_loss = 2 * (K.sigmoid(diff) - 0.5)\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.sigmoid(\n",
    "    #        K.abs(\n",
    "    #            c_labels - c_predictions\n",
    "    #        )\n",
    "    #    )\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    2 * K.sigmoid(\n",
    "    #        K.abs(\n",
    "    #            c_labels - c_predictions\n",
    "    #        )\n",
    "    #    ) - 0.5\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.abs(\n",
    "    #        c_labels - c_predictions\n",
    "    #    )\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    #c_loss = K.sig\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = K.sum(c_labels)\n",
    "    num_non_labels = ANCHOR_WIDTH * ANCHOR_HEIGHT - num_labels\n",
    "    \n",
    "    # Loss matrix for all entries\n",
    "    loss_m_all = keras_binary_crossentropy(c_labels, c_predictions, EPSILON)\n",
    "    \n",
    "    # Loss matrix for the correct label\n",
    "    loss_m_label = keras_binary_crossentropy(c_labels, c_predictions, EPSILON) * c_labels\n",
    "    \n",
    "    # Loss matrix for non labels\n",
    "    loss_m_nonlabel = loss_m_all - loss_m_label\n",
    "    \n",
    "    # Summing and adding weight to label loss\n",
    "    c_loss_label = K.sum(\n",
    "        loss_m_label\n",
    "    ) / BATCHSIZE / num_labels\n",
    "    \n",
    "    # summing and adding weight to non label loss\n",
    "    c_loss_nonlabel = K.sum(\n",
    "        loss_m_nonlabel\n",
    "    ) / BATCHSIZE / num_non_labels\n",
    "    \n",
    "    c_loss = c_loss_label * LABEL_WEIGHT + c_loss_nonlabel * (1 / LABEL_WEIGHT)\n",
    "    \n",
    "    \n",
    "    o_loss_x = K.sum(\n",
    "        #K.pow(true_offset_x - pred_offset_x, 2) * mask_offset_x\n",
    "        #K.abs(true_offset_x - pred_offset_x) * mask_offset_x\n",
    "        K.square((true_offset_x - pred_offset_x) * mask_offset_x)\n",
    "    ) / BATCHSIZE / num_labels\n",
    "    \n",
    "    o_loss_y = K.sum(\n",
    "        #K.pow(true_offset_y - pred_offset_y, 2) * mask_offset_y\n",
    "        #K.abs(true_offset_y - pred_offset_y) * mask_offset_y\n",
    "        K.square((true_offset_y - pred_offset_y) * mask_offset_y)\n",
    "    ) / BATCHSIZE / num_labels\n",
    "    \n",
    "    o_loss = (o_loss_x + o_loss_y) * OFFSET_LOSS_WEIGHT\n",
    "    \n",
    "    # Only on correct label\n",
    "    #o_loss = K.sum(\n",
    "    #    #K.pow(pred_offset - true_offset, 2) * offset_mask\n",
    "    #    (K.pow(\n",
    "    #        true_offset[:, :, :, 0] - pred_offset[:, :, :, 0], 2\n",
    "    #    ) + K.pow(\n",
    "    #        true_offset[:, :, :, 1] - pred_offset[:, :, :, 1], 2\n",
    "    #    ))# * offset_mask\n",
    "    #) / BATCHSIZE / num_labels\n",
    "    \n",
    "    #l2_loss = K.sum(\n",
    "    #    2 * K.sigmoid (\n",
    "    #        K.sqrt(\n",
    "    #            K.pow(y_true_offset[0] - y_true_offset[0], 2) + K.pow(y_true_offset[1] - y_pred_offset[1], 2)\n",
    "    #        )\n",
    "    #    ) - 0.5\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #l2_loss = K.sum(\n",
    "    #    2 * K.sigmoid(\n",
    "    #        K.pow(y_pred_offset - y_true_offset, 2)\n",
    "    #    ) - 0.5\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    \n",
    "    #o_loss = 0\n",
    "    \n",
    "    total_loss = o_loss + c_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "#y_true_test = np.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "#y_pred_test = np.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "#l = loss(y_pred_test, y_true_test)\n",
    "#print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe46c0e7978>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "x = np.linspace(-10, 10, 500)\n",
    "y = 2 * (sigmoid(x) - 0.5)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$−(ylog(p)+(1−y)log(1−p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/48951109/keras-custom-binary-cross-entropy-loss-function-get-nan-as-output-for-loss<br>\n",
    "$max(x, 0) - x * z + log(1 + exp(-abs(x)))$<br>\n",
    "$max(p, 0) - p * y + log(1 + exp(-abs(p)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "c_labels = 1\n",
    "pred_conf = 0\n",
    "\n",
    "diff = abs(c_labels - pred_conf)\n",
    "l = (sigmoid(diff) - 0.5) * 2\n",
    "#print(l)\n",
    "\n",
    "#pred_conf = sigmoid(pred_conf)\n",
    "#ll = -(c_labels * np.log(pred_conf + EPSILON) + (1 - c_labels) * np.log(1-pred_conf + EPSILON))\n",
    "#print(ll)\n",
    "\n",
    "lll = np.max(np.abs(pred_conf), 0) - pred_conf * c_labels + np.log(1 + np.exp(-np.abs(pred_conf)))\n",
    "print(lll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 20, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t = np.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "y_t[:, 10, 10, 0] = 1.0\n",
    "print(y_t.shape)\n",
    "#np.max(np.maximum(-y_t, 0))\n",
    "np.sum(y_t) / BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1 2 3 4 5 6]\n",
      " [1 2 3 4 5 6]\n",
      " [1 2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([[1, 2, 3],\n",
    "                 [1, 2, 3],\n",
    "                 [1, 2, 3]])\n",
    "\n",
    "arr2 = np.array([[4, 5, 6],\n",
    "                 [4, 5, 6],\n",
    "                 [4, 5, 6]])\n",
    "\n",
    "res = np.concatenate((arr1, arr2), axis=-1)\n",
    "print(arr1.shape)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label loss: 0.0\n",
      "Non label loss: 4.605170185988092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.605170185988092"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def np_loss(y_true, y_pred, bs):\n",
    "    #c_predictions = y_pred[:, :, :, 0]\n",
    "    #c_labels = y_true[:, :, :, 0]\n",
    "\n",
    "    c_predictions = y_pred\n",
    "    c_labels = y_true\n",
    "\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = np.sum(c_labels)\n",
    "    num_non_labels = 3 * 3 - num_labels   # ANCHOR_WIDTH and ANCHOR_HEIGHT\n",
    "    \n",
    "    # Loss matrix for all entries\n",
    "    loss_m_all = binary_crossentropy(c_labels, c_predictions, EPSILON)\n",
    "    \n",
    "    # Loss matrix for the correct label\n",
    "    loss_m_label = binary_crossentropy(c_labels, c_predictions, EPSILON) * c_labels\n",
    "    #print(loss_m_label)\n",
    "    \n",
    "    # Loss matrix for non labels\n",
    "    loss_m_nonlabel = loss_m_all - loss_m_label\n",
    "    #print(loss_m_nonlabel)\n",
    "    \n",
    "    # Summing and adding weight to label loss\n",
    "    c_loss_label = np.sum(\n",
    "        loss_m_label\n",
    "    ) / bs / num_labels\n",
    "    \n",
    "    # summing and adding weight to non label loss\n",
    "    c_loss_nonlabel = np.sum(\n",
    "        loss_m_nonlabel\n",
    "    ) / bs / num_non_labels\n",
    "    \n",
    "    print(f\"Label loss: {c_loss_label}\")\n",
    "    print(f\"Non label loss: {c_loss_nonlabel}\")\n",
    "    \n",
    "    c_loss = c_loss_label * LABEL_WEIGHT + c_loss_nonlabel * (1 / LABEL_WEIGHT)\n",
    "    \n",
    "    return c_loss\n",
    "\n",
    "\n",
    "#labels_test = np.copy(labels[-20:])\n",
    "#print(labels_test[:, :, :, 0].shape)\n",
    "#labels_test[:, :, :, 0] = 2\n",
    "#np_loss(labels[:20], labels_test)\n",
    "#print(np_loss(labels[-5:], labels[:5], 5))\n",
    "#print(np_loss(labels[:20], labels[:20], 20))\n",
    "\n",
    "\n",
    "true_test = np.array([[0, 0, 0],\n",
    "                      [0, 1, 0],\n",
    "                      [0, 0, 0]])\n",
    "\n",
    "pred_test = np.array([[0, 1, 0],\n",
    "                      [0, 1, 0],\n",
    "                      [0, 0, 0]])\n",
    "\n",
    "np_loss(true_test, pred_test, bs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.Session() as s:\\n    # Some tensor we want to print the value of\\n    y_true_test = tf.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\\n    y_pred_test = tf.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\\n    s.run(tf.global_variables_initializer())\\n    \\n    l = loss(y_pred_test, y_true_test)\\n    # Add print operation\\n    print(s.run(l))\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "\n",
    "\"\"\"\n",
    "with tf.Session() as s:\n",
    "    # Some tensor we want to print the value of\n",
    "    y_true_test = tf.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    y_pred_test = tf.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    \n",
    "    l = loss(y_pred_test, y_true_test)\n",
    "    # Add print operation\n",
    "    print(s.run(l))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.841361487904734\n"
     ]
    }
   ],
   "source": [
    "c_labels = 0\n",
    "c_predictions = 1\n",
    "c_loss = (c_labels * (-np.log(c_predictions + EPSILON))) + (1-c_labels) * (-np.log(1-c_predictions + EPSILON))\n",
    "print(c_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(inputs=input_layer, outputs=preds)\n",
    "#model.compile(loss='mse', optimizer='adam')\n",
    "#model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    with open(annotation, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    gt = [(None, None)] * len(lines)\n",
    "    \n",
    "    for l in lines:\n",
    "        obj = l.split(',')\n",
    "        pic_id = int(obj[0].split('.')[0])\n",
    "        x = int(obj[1])\n",
    "        y = int(obj[2])\n",
    "        \n",
    "        gt[pic_id] = (x, y)\n",
    "\n",
    "    images = []\n",
    "    \n",
    "    for fi in os.listdir(DATA_DIR):\n",
    "        if not fi.endswith('jpg'):\n",
    "            continue\n",
    "        im = cv2.imread(os.path.join(DATA_DIR, fi))\n",
    "        images.append(im)\n",
    "    \n",
    "    return gt, images\n",
    "\n",
    "labels_old, images_old = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 3)\n",
      "1\n",
      "0.0125\n",
      "0.0\n",
      "[[1. 5. 0.]]\n",
      "(20, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "def closest_anchor_map(x, y, anchor_coords):\n",
    "    \"\"\" Create a anchor_height x anchor_width x 3 map.\n",
    "        First entry is 1 if the anchor point is closest to true point. Zero otherwise.\n",
    "        Second is x offset.\n",
    "        Third is y offset. \"\"\"\n",
    "    closest = 10000\n",
    "    closest_x = None\n",
    "    closest_y = None\n",
    "    closest_x_offset = None\n",
    "    closest_y_offset = None\n",
    "    \n",
    "    res = np.zeros((ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    for ix in range(ANCHOR_HEIGHT):\n",
    "        for iy in range(ANCHOR_WIDTH):\n",
    "            p_x, p_y = anchor_coords[ix, iy]\n",
    "            dist = np.sqrt( (x - p_x)**2 + (y - p_y)**2 )\n",
    "            #res[ix, iy, 1:] = (x - p_x, y - p_y)\n",
    "            if dist < closest:\n",
    "                closest = dist\n",
    "                closest_x = ix\n",
    "                closest_y = iy\n",
    "                closest_x_offset = (x - p_x)\n",
    "                closest_y_offset = (y - p_y)\n",
    "    \n",
    "    #print(f\"({closest_x}, {closest_y}) -> {anchor_coords[closest_x, closest_y]}\")\n",
    "    res[closest_x, closest_y, 0] = 1\n",
    "    res[closest_x, closest_y, 1:] = (closest_x_offset, closest_y_offset)\n",
    "    \n",
    "    return res\n",
    "        \n",
    "test_map = closest_anchor_map(20, 30, anchs)\n",
    "print(test_map.shape)\n",
    "print(np.count_nonzero(test_map[:,:, 0]))\n",
    "print(np.mean(test_map[:, :, 1]))\n",
    "print(np.mean(test_map[:, :, 2]))\n",
    "anc_indicies = np.where(test_map[:, :, 0] == test_map[:, :, 0].max())\n",
    "print(test_map[anc_indicies[0], anc_indicies[1]])\n",
    "anchor_point = test_map[anc_indicies[0], anc_indicies[1]][:,1:][0]\n",
    "print(anchs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ea8591a1574108b609010ad5e9bc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363607be629b4623b8262be773bcdc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1000, 20, 20, 3)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "def load_data_with_anchors():\n",
    "    # load images\n",
    "    # labels will be:\n",
    "    #   anchor_height x anchor_width x 3\n",
    "    #     the last 3 entries is: 1 if closest gridpoint to a point. x and y offsets to closest point.\n",
    "    with open(annotation, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    gt = np.zeros((len(lines), ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    gt_clean = [(None, None)] * len(lines)\n",
    "    images = np.zeros((len(lines), HEIGHT, WIDTH, 3))#, dtype=np.uint8)\n",
    "    \n",
    "    for c, l in enumerate(tqdm(lines)):\n",
    "        obj = l.split(',')\n",
    "        pic_id = int(obj[0].split('.')[0])\n",
    "        x = int(obj[1])\n",
    "        y = int(obj[2])\n",
    "        \n",
    "        gt[pic_id, :, :] = closest_anchor_map(x, y, anchs)\n",
    "        gt_clean[pic_id] = (x, y)\n",
    "    \n",
    "    #images = []\n",
    "    \n",
    "    for fi in tqdm(os.listdir(DATA_DIR)):\n",
    "        if not fi.endswith('jpg'):\n",
    "            continue\n",
    "        im = cv2.imread(os.path.join(DATA_DIR, fi))\n",
    "        #images.append(im)\n",
    "        i = int(fi.split('.')[0])\n",
    "        images[i] = im / 255.0\n",
    "    \n",
    "    images = np.array(images)\n",
    "    \n",
    "    return gt, gt_clean, images\n",
    "        \n",
    "labels, labels_clean, images = load_data_with_anchors()\n",
    "print(labels.shape)\n",
    "print(images[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels[5][0][0])\n",
    "np.max(images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it everythin gets loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max label index: 12\n",
      "Values above threshold: 1\n",
      "Raw offset: (-7.0, 5.0)\n",
      "[[ 198  121 -300  180]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 198,  121, -300,  180], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_points_from_prediction(pred, threshold=1.0, do_scale=True):\n",
    "    \"\"\"\n",
    "    pred is a prediction map in the shape (ANCHOR_HEIGHT, ANCHOR_WIDTH, 3)\n",
    "    \"\"\"\n",
    "    # Get all points with a confidence above threshold\n",
    "    label_indicies = np.where(pred[:, :, 0] >= threshold)\n",
    "    num_points = len(label_indicies[0])\n",
    "    print(f\"max label index: {np.max(label_indicies)}\")\n",
    "    print(f\"Values above threshold: {num_points}\")\n",
    "    \n",
    "    points = np.zeros((num_points, 4))#, dtype=np.int32)\n",
    "    \n",
    "    # Loop through all anchor points\n",
    "    for c, (x_anchor, y_anchor) in enumerate(zip(label_indicies[0], label_indicies[1])):\n",
    "        #print(x_anchor)\n",
    "        # when anchor location is known, the location of the closest anchor in the actual image can be found\n",
    "        x_without_offset, y_without_offset = anchs[x_anchor, y_anchor]\n",
    "        #print(f\"Anchor: ({x_anchor, y_anchor})\")\n",
    "        \n",
    "        # The offset can then be extracted from the labels\n",
    "        (x_offset, y_offset) = pred[label_indicies[0], label_indicies[1]][0][1:]\n",
    "        print(f\"Raw offset: ({x_offset}, {y_offset})\")\n",
    "        if do_scale:\n",
    "            x_offset = 2 * (x_offset - 0.5) * OFFSET_WEIGHT\n",
    "            y_offset = 2 * (y_offset - 0.5) * OFFSET_WEIGHT\n",
    "        #print(f\"({x_offset}, {y_offset})\")\n",
    "        \n",
    "        # and the final point calculated\n",
    "        #actual_x = int(x_without_offset + x_offset)\n",
    "        #actual_y= int(y_without_offset + y_offset)\n",
    "\n",
    "        points[c] = (x_without_offset, y_without_offset, x_offset, y_offset)\n",
    "    \n",
    "    return points.astype(np.int32)\n",
    "    \n",
    "\n",
    "p = labels[3]\n",
    "#print(p.shape)\n",
    "pp = get_all_points_from_prediction(p)\n",
    "print(pp)\n",
    "print(\"\")\n",
    "pp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 30]\n",
      "Max values in labels: 1\n",
      "max label index: 12\n",
      "Values above threshold: 1\n",
      "Raw offset: (6.0, -7.0)\n",
      "Anchor: (152, 198)\n",
      "[[ 1.  6. -7.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Red is center, green is closest anchor')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "im = np.copy(images[1]) * 255.0\n",
    "#im = im.astype(np.uint8)\n",
    "\n",
    "im_anchors = np.copy(images[1]) * 255.0\n",
    "#im = im.astype(np.uint8)\n",
    "\n",
    "print(anchs[1, 1])\n",
    "\n",
    "# draw all anchors\n",
    "for anc_x in range(ANCHOR_HEIGHT):\n",
    "    for anc_y in range(ANCHOR_WIDTH):\n",
    "        cv2.circle(im_anchors, (anchs[anc_x, anc_y][0], anchs[anc_x, anc_y][1]), 1, (128, 128, 128), thickness=1)\n",
    "\n",
    "\n",
    "\n",
    "# Get labels with max value, we know this to only be one in the labels\n",
    "label_indicies = np.where(labels[index, :, :, 0] == labels[index, :, :, 0].max())\n",
    "print(f\"Max values in labels: {len(label_indicies[0])}\")\n",
    "\n",
    "# Get location in the anchor\n",
    "#x_anchor, y_anchor = label_indicies\n",
    "# when anchor location is known, the location of the closest anchor in the actual image can be found\n",
    "#x_without_offset, y_without_offset = anchs[x_anchor[0], y_anchor[0]]\n",
    "\"\"\"\n",
    "# The offset can then be extracted from the labels\n",
    "(x_offset, y_offset) = labels[index, label_indicies[0], label_indicies[1]][0][1:]\n",
    "# and the final point calculated\n",
    "actual_x = int(x_without_offset + x_offset)\n",
    "actual_y= int(y_without_offset + y_offset)\n",
    "print(f\"Actual point: ({actual_x}, {actual_y})\")\n",
    "\"\"\"\n",
    "actual_x_anch, actual_y_anch, actual_x_off, actual_y_off = get_all_points_from_prediction(labels[index],\n",
    "                                                                                          do_scale=False)[0]\n",
    "actual_point = (actual_x_anch + actual_x_off, actual_y_anch + actual_y_off)\n",
    "\n",
    "cv2.circle(im, (actual_y_anch, actual_x_anch), 2, (0, 255, 0), thickness=2)\n",
    "cv2.circle(im, (actual_point[1], actual_point[0]), 2, (255, 0, 0), thickness=2)\n",
    "print(f\"Anchor: ({actual_x_anch}, {actual_y_anch})\")\n",
    "print(labels[index, label_indicies[0], label_indicies[1]])\n",
    "\n",
    "f, subs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "subs[0].imshow(im_anchors.astype(np.uint8))\n",
    "subs[0].set_title(\"Anchor points\")\n",
    "subs[1].imshow(im.astype(np.uint8))\n",
    "subs[1].set_title(\"Red is center, green is closest anchor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.array(labels)#.reshape(1, 100, 2)\n",
    "#print(labels.shape)\n",
    "#print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c, i in enumerate(images):\n",
    "#    model.fit(i.reshape(1, 320, 320, 3), labels[c].reshape(1, 2), epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(images.reshape(-1, 320, 320, 3), labels.reshape(-1, 2), batch_size=10, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def loss(y_true, y_pred):\n",
    "#    return K.sqrt(K.sum(K.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintInfo(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print(f\"Learning rate with decay: {K.eval(lr_with_decay)}\")\n",
    "        #print(f\"lr={K.eval(lr)}, decay={K.eval(decay)}\")\n",
    "        print(\"\")\n",
    "        \n",
    "print_info = PrintInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 320, 320, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 160, 160, 64) 1792        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 80, 80, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire1/squeeze1x1 (Conv2D)       (None, 80, 80, 32)   2080        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire1/expand1x1 (Conv2D)        (None, 80, 80, 128)  4224        fire1/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire1/expand3x3 (Conv2D)        (None, 80, 80, 128)  36992       fire1/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80, 80, 256)  0           fire1/expand1x1[0][0]            \n",
      "                                                                 fire1/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 40, 40, 256)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 40, 40, 48)   12336       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 40, 40, 192)  9408        fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 40, 40, 192)  83136       fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 40, 40, 384)  0           fire3/expand1x1[0][0]            \n",
      "                                                                 fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 40, 40, 48)   18480       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 40, 40, 192)  9408        fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 40, 40, 192)  83136       fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 40, 40, 384)  0           fire4/expand1x1[0][0]            \n",
      "                                                                 fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 20, 20, 384)  0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pred_conf (Conv2D)              (None, 20, 20, 1)    385         pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pred_offset (Conv2D)            (None, 20, 20, 2)    770         pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 20, 20, 3)    0           pred_conf[0][0]                  \n",
      "                                                                 pred_offset[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 262,147\n",
      "Trainable params: 262,147\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=1e-3, decay=1e-5) #, clipnorm=1.0)\n",
    "#opt = optimizers.RMSprop(lr=0.001,  clipnorm=1.0)\n",
    "#opt = optimizers.SGD(lr=0.01, decay=0.001, momentum=0.9, nesterov=False)\n",
    "#opt = optimizers.Adagrad(lr=0.001, clipnorm=1.0)\n",
    "#opt =optimizers.SGD()\n",
    "model.compile(loss=loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 1.7668\n",
      "Learning rate with decay: 0.0009996801381930709\n",
      "\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 1.2603\n",
      "Learning rate with decay: 0.0009993604617193341\n",
      "\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 1.0848\n",
      "Learning rate with decay: 0.000999041018076241\n",
      "\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.9150\n",
      "Learning rate with decay: 0.0009987216908484697\n",
      "\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.8492\n",
      "Learning rate with decay: 0.000998402596451342\n",
      "\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.7369\n",
      "Learning rate with decay: 0.0009980837348848581\n",
      "\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.7512\n",
      "Learning rate with decay: 0.0009977651061490178\n",
      "\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.7320\n",
      "Learning rate with decay: 0.0009974465938284993\n",
      "\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6602\n",
      "Learning rate with decay: 0.0009971283143386245\n",
      "\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6447\n",
      "Learning rate with decay: 0.0009968101512640715\n",
      "\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6804\n",
      "Learning rate with decay: 0.000996492337435484\n",
      "\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6311\n",
      "Learning rate with decay: 0.00099617475643754\n",
      "\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6388\n",
      "Learning rate with decay: 0.000995857291854918\n",
      "\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6190\n",
      "Learning rate with decay: 0.0009955400601029396\n",
      "\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6109\n",
      "Learning rate with decay: 0.0009952230611816049\n",
      "\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6041\n",
      "Learning rate with decay: 0.0009949060622602701\n",
      "\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6271\n",
      "Learning rate with decay: 0.0009945895290002227\n",
      "\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.6146\n",
      "Learning rate with decay: 0.000994273112155497\n",
      "\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5846\n",
      "Learning rate with decay: 0.0009939568117260933\n",
      "\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5861\n",
      "Learning rate with decay: 0.0009936407441273332\n",
      "\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5853\n",
      "Learning rate with decay: 0.0009933249093592167\n",
      "\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5957\n",
      "Learning rate with decay: 0.000993009191006422\n",
      "\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5829\n",
      "Learning rate with decay: 0.0009926938218995929\n",
      "\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5600\n",
      "Learning rate with decay: 0.0009923785692080855\n",
      "\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5484\n",
      "Learning rate with decay: 0.0009920635493472219\n",
      "\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5208\n",
      "Learning rate with decay: 0.0009917487623170018\n",
      "\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5028\n",
      "Learning rate with decay: 0.0009914339752867818\n",
      "\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5016\n",
      "Learning rate with decay: 0.000991119653917849\n",
      "\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5027\n",
      "Learning rate with decay: 0.0009908054489642382\n",
      "\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.5386\n",
      "Learning rate with decay: 0.0009904912440106273\n",
      "\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.5243\n",
      "Learning rate with decay: 0.0009901775047183037\n",
      "\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4938\n",
      "Learning rate with decay: 0.000989863881841302\n",
      "\n",
      "Epoch 33/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4975\n",
      "Learning rate with decay: 0.000989550375379622\n",
      "\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4768\n",
      "Learning rate with decay: 0.0009892371017485857\n",
      "\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.5139\n",
      "Learning rate with decay: 0.000988924177363515\n",
      "\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4775\n",
      "Learning rate with decay: 0.000988611252978444\n",
      "\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4810\n",
      "Learning rate with decay: 0.000988298561424017\n",
      "\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.5060\n",
      "Learning rate with decay: 0.0009879862191155553\n",
      "\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4768\n",
      "Learning rate with decay: 0.0009876738768070936\n",
      "\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4667\n",
      "Learning rate with decay: 0.0009873618837445974\n",
      "\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4617\n",
      "Learning rate with decay: 0.0009870498906821012\n",
      "\n",
      "Epoch 42/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4516\n",
      "Learning rate with decay: 0.0009867382468655705\n",
      "\n",
      "Epoch 43/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4405\n",
      "Learning rate with decay: 0.0009864268358796835\n",
      "\n",
      "Epoch 44/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4468\n",
      "Learning rate with decay: 0.0009861155413091183\n",
      "\n",
      "Epoch 45/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4584\n",
      "Learning rate with decay: 0.0009858044795691967\n",
      "\n",
      "Epoch 46/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4506\n",
      "Learning rate with decay: 0.0009854936506599188\n",
      "\n",
      "Epoch 47/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4489\n",
      "Learning rate with decay: 0.0009851828217506409\n",
      "\n",
      "Epoch 48/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4419\n",
      "Learning rate with decay: 0.0009848724585026503\n",
      "\n",
      "Epoch 49/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4617\n",
      "Learning rate with decay: 0.0009845622116699815\n",
      "\n",
      "Epoch 50/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4594\n",
      "Learning rate with decay: 0.0009842519648373127\n",
      "\n",
      "Epoch 51/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4349\n",
      "Learning rate with decay: 0.0009839420672506094\n",
      "\n",
      "Epoch 52/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4425\n",
      "Learning rate with decay: 0.0009836324024945498\n",
      "\n",
      "Epoch 53/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4344\n",
      "Learning rate with decay: 0.000983322854153812\n",
      "\n",
      "Epoch 54/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4209\n",
      "Learning rate with decay: 0.0009830135386437178\n",
      "\n",
      "Epoch 55/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4181\n",
      "Learning rate with decay: 0.0009827043395489454\n",
      "\n",
      "Epoch 56/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4204\n",
      "Learning rate with decay: 0.0009823954897001386\n",
      "\n",
      "Epoch 57/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4185\n",
      "Learning rate with decay: 0.0009820867562666535\n",
      "\n",
      "Epoch 58/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4324\n",
      "Learning rate with decay: 0.0009817781392484903\n",
      "\n",
      "Epoch 59/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4187\n",
      "Learning rate with decay: 0.0009814698714762926\n",
      "\n",
      "Epoch 60/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4104\n",
      "Learning rate with decay: 0.0009811617201194167\n",
      "\n",
      "Epoch 61/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4212\n",
      "Learning rate with decay: 0.0009808536851778626\n",
      "\n",
      "Epoch 62/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4273\n",
      "Learning rate with decay: 0.000980545999482274\n",
      "\n",
      "Epoch 63/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4487\n",
      "Learning rate with decay: 0.0009802384302020073\n",
      "\n",
      "Epoch 64/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4188\n",
      "Learning rate with decay: 0.0009799309773370624\n",
      "\n",
      "Epoch 65/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4056\n",
      "Learning rate with decay: 0.000979623873718083\n",
      "\n",
      "Epoch 66/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4130\n",
      "Learning rate with decay: 0.0009793168865144253\n",
      "\n",
      "Epoch 67/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4031\n",
      "Learning rate with decay: 0.0009790100157260895\n",
      "\n",
      "Epoch 68/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3953\n",
      "Learning rate with decay: 0.0009787034941837192\n",
      "\n",
      "Epoch 69/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3970\n",
      "Learning rate with decay: 0.0009783970890566707\n",
      "\n",
      "Epoch 70/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3925\n",
      "Learning rate with decay: 0.000978090800344944\n",
      "\n",
      "Epoch 71/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3959\n",
      "Learning rate with decay: 0.000977784744463861\n",
      "\n",
      "Epoch 72/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4003\n",
      "Learning rate with decay: 0.0009774789214134216\n",
      "\n",
      "Epoch 73/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3908\n",
      "Learning rate with decay: 0.000977173214778304\n",
      "\n",
      "Epoch 74/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3877\n",
      "Learning rate with decay: 0.000976867857389152\n",
      "\n",
      "Epoch 75/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3860\n",
      "Learning rate with decay: 0.0009765625\n",
      "\n",
      "Epoch 76/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3929\n",
      "Learning rate with decay: 0.0009762574336491525\n",
      "\n",
      "Epoch 77/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3879\n",
      "Learning rate with decay: 0.0009759526001289487\n",
      "\n",
      "Epoch 78/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3819\n",
      "Learning rate with decay: 0.0009756478248164058\n",
      "\n",
      "Epoch 79/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3839\n",
      "Learning rate with decay: 0.0009753433405421674\n",
      "\n",
      "Epoch 80/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3880\n",
      "Learning rate with decay: 0.0009750390890985727\n",
      "\n",
      "Epoch 81/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3808\n",
      "Learning rate with decay: 0.000974734895862639\n",
      "\n",
      "Epoch 82/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4058\n",
      "Learning rate with decay: 0.0009744309936650097\n",
      "\n",
      "Epoch 83/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3857\n",
      "Learning rate with decay: 0.0009741272660903633\n",
      "\n",
      "Epoch 84/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3766\n",
      "Learning rate with decay: 0.0009738236549310386\n",
      "\n",
      "Epoch 85/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3809\n",
      "Learning rate with decay: 0.0009735203348100185\n",
      "\n",
      "Epoch 86/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3759\n",
      "Learning rate with decay: 0.0009732171893119812\n",
      "\n",
      "Epoch 87/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3989\n",
      "Learning rate with decay: 0.0009729141020216048\n",
      "\n",
      "Epoch 88/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3824\n",
      "Learning rate with decay: 0.0009726113639771938\n",
      "\n",
      "Epoch 89/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3782\n",
      "Learning rate with decay: 0.0009723086259327829\n",
      "\n",
      "Epoch 90/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3694\n",
      "Learning rate with decay: 0.0009720062371343374\n",
      "\n",
      "Epoch 91/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3702\n",
      "Learning rate with decay: 0.0009717040811665356\n",
      "\n",
      "Epoch 92/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3837\n",
      "Learning rate with decay: 0.0009714019251987338\n",
      "\n",
      "Epoch 93/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3840\n",
      "Learning rate with decay: 0.0009711001184768975\n",
      "\n",
      "Epoch 94/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3787\n",
      "Learning rate with decay: 0.0009707984863780439\n",
      "\n",
      "Epoch 95/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3647\n",
      "Learning rate with decay: 0.0009704969124868512\n",
      "\n",
      "Epoch 96/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3641\n",
      "Learning rate with decay: 0.0009701956296339631\n",
      "\n",
      "Epoch 97/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3594\n",
      "Learning rate with decay: 0.0009698945796117187\n",
      "\n",
      "Epoch 98/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3855\n",
      "Learning rate with decay: 0.0009695935877971351\n",
      "\n",
      "Epoch 99/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3724\n",
      "Learning rate with decay: 0.0009692928870208561\n",
      "\n",
      "Epoch 100/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3589\n",
      "Learning rate with decay: 0.0009689923608675599\n",
      "\n",
      "Epoch 101/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3660\n",
      "Learning rate with decay: 0.0009686918929219246\n",
      "\n",
      "Epoch 102/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3540\n",
      "Learning rate with decay: 0.0009683917742222548\n",
      "\n",
      "Epoch 103/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3632\n",
      "Learning rate with decay: 0.0009680918301455677\n",
      "\n",
      "Epoch 104/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3607\n",
      "Learning rate with decay: 0.0009677919442765415\n",
      "\n",
      "Epoch 105/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3536\n",
      "Learning rate with decay: 0.0009674923494458199\n",
      "\n",
      "Epoch 106/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3580\n",
      "Learning rate with decay: 0.0009671928128227592\n",
      "\n",
      "Epoch 107/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3510\n",
      "Learning rate with decay: 0.0009668936254456639\n",
      "\n",
      "Epoch 108/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3460\n",
      "Learning rate with decay: 0.0009665945544838905\n",
      "\n",
      "Epoch 109/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3463\n",
      "Learning rate with decay: 0.000966295599937439\n",
      "\n",
      "Epoch 110/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3444\n",
      "Learning rate with decay: 0.000965996936429292\n",
      "\n",
      "Epoch 111/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3496\n",
      "Learning rate with decay: 0.0009656985057517886\n",
      "\n",
      "Epoch 112/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3554\n",
      "Learning rate with decay: 0.0009654000750742853\n",
      "\n",
      "Epoch 113/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3491\n",
      "Learning rate with decay: 0.0009651019936427474\n",
      "\n",
      "Epoch 114/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3600\n",
      "Learning rate with decay: 0.0009648040286265314\n",
      "\n",
      "Epoch 115/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3616\n",
      "Learning rate with decay: 0.0009645061800256371\n",
      "\n",
      "Epoch 116/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3429\n",
      "Learning rate with decay: 0.0009642086224630475\n",
      "\n",
      "Epoch 117/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3474\n",
      "Learning rate with decay: 0.0009639112395234406\n",
      "\n",
      "Epoch 118/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3359\n",
      "Learning rate with decay: 0.0009636139729991555\n",
      "\n",
      "Epoch 119/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3403\n",
      "Learning rate with decay: 0.0009633169393055141\n",
      "\n",
      "Epoch 120/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3451\n",
      "Learning rate with decay: 0.0009630200220271945\n",
      "\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3425\n",
      "Learning rate with decay: 0.0009627233957871795\n",
      "\n",
      "Epoch 122/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3430\n",
      "Learning rate with decay: 0.0009624269441701472\n",
      "\n",
      "Epoch 123/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3312\n",
      "Learning rate with decay: 0.0009621305507607758\n",
      "\n",
      "Epoch 124/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3560\n",
      "Learning rate with decay: 0.000961834448389709\n",
      "\n",
      "Epoch 125/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3488\n",
      "Learning rate with decay: 0.0009615385206416249\n",
      "\n",
      "Epoch 126/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3348\n",
      "Learning rate with decay: 0.0009612427093088627\n",
      "\n",
      "Epoch 127/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3299\n",
      "Learning rate with decay: 0.0009609471308067441\n",
      "\n",
      "Epoch 128/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3229\n",
      "Learning rate with decay: 0.0009606517851352692\n",
      "\n",
      "Epoch 129/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3181\n",
      "Learning rate with decay: 0.0009603564976714551\n",
      "\n",
      "Epoch 130/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3299\n",
      "Learning rate with decay: 0.0009600615012459457\n",
      "\n",
      "Epoch 131/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3300\n",
      "Learning rate with decay: 0.000959766679443419\n",
      "\n",
      "Epoch 132/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3254\n",
      "Learning rate with decay: 0.0009594719158485532\n",
      "\n",
      "Epoch 133/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3294\n",
      "Learning rate with decay: 0.000959177443291992\n",
      "\n",
      "Epoch 134/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3201\n",
      "Learning rate with decay: 0.0009588830871507525\n",
      "\n",
      "Epoch 135/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3358\n",
      "Learning rate with decay: 0.0009585889638401568\n",
      "\n",
      "Epoch 136/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3364\n",
      "Learning rate with decay: 0.0009582950733602047\n",
      "\n",
      "Epoch 137/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3221\n",
      "Learning rate with decay: 0.0009580012410879135\n",
      "\n",
      "Epoch 138/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3156\n",
      "Learning rate with decay: 0.000957707641646266\n",
      "\n",
      "Epoch 139/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3114\n",
      "Learning rate with decay: 0.0009574142750352621\n",
      "\n",
      "Epoch 140/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3116\n",
      "Learning rate with decay: 0.0009571209666319191\n",
      "\n",
      "Epoch 141/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3182\n",
      "Learning rate with decay: 0.0009568279492668808\n",
      "\n",
      "Epoch 142/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3052\n",
      "Learning rate with decay: 0.0009565351065248251\n",
      "\n",
      "Epoch 143/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3135\n",
      "Learning rate with decay: 0.0009562423801980913\n",
      "\n",
      "Epoch 144/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3022\n",
      "Learning rate with decay: 0.0009559498867020011\n",
      "\n",
      "Epoch 145/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3069\n",
      "Learning rate with decay: 0.0009556575678288937\n",
      "\n",
      "Epoch 146/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3076\n",
      "Learning rate with decay: 0.0009553653653711081\n",
      "\n",
      "Epoch 147/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2989\n",
      "Learning rate with decay: 0.0009550733957439661\n",
      "\n",
      "Epoch 148/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3002\n",
      "Learning rate with decay: 0.0009547816589474678\n",
      "\n",
      "Epoch 149/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3240\n",
      "Learning rate with decay: 0.0009544899221509695\n",
      "\n",
      "Epoch 150/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3145\n",
      "Learning rate with decay: 0.0009541985346004367\n",
      "\n",
      "Epoch 151/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2961\n",
      "Learning rate with decay: 0.0009539072052575648\n",
      "\n",
      "Epoch 152/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2945\n",
      "Learning rate with decay: 0.0009536161669529974\n",
      "\n",
      "Epoch 153/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2970\n",
      "Learning rate with decay: 0.0009533252450637519\n",
      "\n",
      "Epoch 154/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2959\n",
      "Learning rate with decay: 0.0009530344395898283\n",
      "\n",
      "Epoch 155/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2907\n",
      "Learning rate with decay: 0.0009527439251542091\n",
      "\n",
      "Epoch 156/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2888\n",
      "Learning rate with decay: 0.0009524535853415728\n",
      "\n",
      "Epoch 157/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2946\n",
      "Learning rate with decay: 0.0009521633037365973\n",
      "\n",
      "Epoch 158/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3005\n",
      "Learning rate with decay: 0.0009518733131699264\n",
      "\n",
      "Epoch 159/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2907\n",
      "Learning rate with decay: 0.0009515834972262383\n",
      "\n",
      "Epoch 160/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2913\n",
      "Learning rate with decay: 0.0009512937976978719\n",
      "\n",
      "Epoch 161/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3056\n",
      "Learning rate with decay: 0.0009510043310001493\n",
      "\n",
      "Epoch 162/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2885\n",
      "Learning rate with decay: 0.0009507150389254093\n",
      "\n",
      "Epoch 163/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2730\n",
      "Learning rate with decay: 0.0009504258050583303\n",
      "\n",
      "Epoch 164/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2816\n",
      "Learning rate with decay: 0.0009501368622295558\n",
      "\n",
      "Epoch 165/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2772\n",
      "Learning rate with decay: 0.000949848152231425\n",
      "\n",
      "Epoch 166/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2763\n",
      "Learning rate with decay: 0.0009495594422332942\n",
      "\n",
      "Epoch 167/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2791\n",
      "Learning rate with decay: 0.000949271023273468\n",
      "\n",
      "Epoch 168/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2795\n",
      "Learning rate with decay: 0.0009489826625213027\n",
      "\n",
      "Epoch 169/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2732\n",
      "Learning rate with decay: 0.0009486946510151029\n",
      "\n",
      "Epoch 170/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2680\n",
      "Learning rate with decay: 0.0009484067559242249\n",
      "\n",
      "Epoch 171/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2699\n",
      "Learning rate with decay: 0.0009481189190410078\n",
      "\n",
      "Epoch 172/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2674\n",
      "Learning rate with decay: 0.0009478314314037561\n",
      "\n",
      "Epoch 173/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2656\n",
      "Learning rate with decay: 0.0009475440601818264\n",
      "\n",
      "Epoch 174/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2724\n",
      "Learning rate with decay: 0.0009472567471675575\n",
      "\n",
      "Epoch 175/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2698\n",
      "Learning rate with decay: 0.0009469697251915932\n",
      "\n",
      "Epoch 176/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2653\n",
      "Learning rate with decay: 0.0009466829360462725\n",
      "\n",
      "Epoch 177/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2812\n",
      "Learning rate with decay: 0.0009463961469009519\n",
      "\n",
      "Epoch 178/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2679\n",
      "Learning rate with decay: 0.0009461096487939358\n",
      "\n",
      "Epoch 179/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2668\n",
      "Learning rate with decay: 0.0009458233253099024\n",
      "\n",
      "Epoch 180/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2604\n",
      "Learning rate with decay: 0.0009455371182411909\n",
      "\n",
      "Epoch 181/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2642\n",
      "Learning rate with decay: 0.000945251144003123\n",
      "\n",
      "Epoch 182/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2608\n",
      "Learning rate with decay: 0.0009449652279727161\n",
      "\n",
      "Epoch 183/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2607\n",
      "Learning rate with decay: 0.0009446796029806137\n",
      "\n",
      "Epoch 184/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2585\n",
      "Learning rate with decay: 0.0009443941526114941\n",
      "\n",
      "Epoch 185/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2548\n",
      "Learning rate with decay: 0.0009441087604500353\n",
      "\n",
      "Epoch 186/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2550\n",
      "Learning rate with decay: 0.0009438236593268812\n",
      "\n",
      "Epoch 187/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2579\n",
      "Learning rate with decay: 0.0009435387328267097\n",
      "\n",
      "Epoch 188/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2504\n",
      "Learning rate with decay: 0.0009432538645341992\n",
      "\n",
      "Epoch 189/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2624\n",
      "Learning rate with decay: 0.0009429692872799933\n",
      "\n",
      "Epoch 190/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2600\n",
      "Learning rate with decay: 0.0009426848264411092\n",
      "\n",
      "Epoch 191/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2538\n",
      "Learning rate with decay: 0.0009424004820175469\n",
      "\n",
      "Epoch 192/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2629\n",
      "Learning rate with decay: 0.0009421164286322892\n",
      "\n",
      "Epoch 193/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2573\n",
      "Learning rate with decay: 0.0009418324916623533\n",
      "\n",
      "Epoch 194/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2493\n",
      "Learning rate with decay: 0.0009415486711077392\n",
      "\n",
      "Epoch 195/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2460\n",
      "Learning rate with decay: 0.0009412651415914297\n",
      "\n",
      "Epoch 196/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2392\n",
      "Learning rate with decay: 0.0009409816120751202\n",
      "\n",
      "Epoch 197/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2396\n",
      "Learning rate with decay: 0.0009406984318047762\n",
      "\n",
      "Epoch 198/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2419\n",
      "Learning rate with decay: 0.000940415367949754\n",
      "\n",
      "Epoch 199/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2485\n",
      "Learning rate with decay: 0.0009401323623023927\n",
      "\n",
      "Epoch 200/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2501\n",
      "Learning rate with decay: 0.000939849647693336\n",
      "\n",
      "Epoch 201/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2485\n",
      "Learning rate with decay: 0.000939567107707262\n",
      "\n",
      "Epoch 202/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2683\n",
      "Learning rate with decay: 0.000939284625928849\n",
      "\n",
      "Epoch 203/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2615\n",
      "Learning rate with decay: 0.0009390024351887405\n",
      "\n",
      "Epoch 204/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2461\n",
      "Learning rate with decay: 0.0009387204190716147\n",
      "\n",
      "Epoch 205/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2507\n",
      "Learning rate with decay: 0.0009384384611621499\n",
      "\n",
      "Epoch 206/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2437\n",
      "Learning rate with decay: 0.0009381567360833287\n",
      "\n",
      "Epoch 207/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2307\n",
      "Learning rate with decay: 0.0009378752438351512\n",
      "\n",
      "Epoch 208/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2366\n",
      "Learning rate with decay: 0.0009375937515869737\n",
      "\n",
      "Epoch 209/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2307\n",
      "Learning rate with decay: 0.0009373126085847616\n",
      "\n",
      "Epoch 210/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2278\n",
      "Learning rate with decay: 0.0009370315819978714\n",
      "\n",
      "Epoch 211/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2307\n",
      "Learning rate with decay: 0.0009367506136186421\n",
      "\n",
      "Epoch 212/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2245\n",
      "Learning rate with decay: 0.0009364699362777174\n",
      "\n",
      "Epoch 213/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2234\n",
      "Learning rate with decay: 0.0009361893171444535\n",
      "\n",
      "Epoch 214/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2317\n",
      "Learning rate with decay: 0.0009359089890494943\n",
      "\n",
      "Epoch 215/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2338\n",
      "Learning rate with decay: 0.0009356288355775177\n",
      "\n",
      "Epoch 216/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2308\n",
      "Learning rate with decay: 0.0009353486821055412\n",
      "\n",
      "Epoch 217/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2290\n",
      "Learning rate with decay: 0.0009350688778795302\n",
      "\n",
      "Epoch 218/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2241\n",
      "Learning rate with decay: 0.000934789190068841\n",
      "\n",
      "Epoch 219/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2165\n",
      "Learning rate with decay: 0.0009345095604658127\n",
      "\n",
      "Epoch 220/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2320\n",
      "Learning rate with decay: 0.000934230221901089\n",
      "\n",
      "Epoch 221/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2300\n",
      "Learning rate with decay: 0.000933951057959348\n",
      "\n",
      "Epoch 222/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2181\n",
      "Learning rate with decay: 0.0009336719522252679\n",
      "\n",
      "Epoch 223/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2216\n",
      "Learning rate with decay: 0.0009333931375294924\n",
      "\n",
      "Epoch 224/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2185\n",
      "Learning rate with decay: 0.0009331144392490387\n",
      "\n",
      "Epoch 225/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2187\n",
      "Learning rate with decay: 0.0009328358573839068\n",
      "\n",
      "Epoch 226/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2132\n",
      "Learning rate with decay: 0.0009325575083494186\n",
      "\n",
      "Epoch 227/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2182\n",
      "Learning rate with decay: 0.0009322793339379132\n",
      "\n",
      "Epoch 228/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2217\n",
      "Learning rate with decay: 0.0009320012177340686\n",
      "\n",
      "Epoch 229/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2163\n",
      "Learning rate with decay: 0.0009317233925685287\n",
      "\n",
      "Epoch 230/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2095\n",
      "Learning rate with decay: 0.0009314456256106496\n",
      "\n",
      "Epoch 231/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2196\n",
      "Learning rate with decay: 0.0009311680914834142\n",
      "\n",
      "Epoch 232/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2103\n",
      "Learning rate with decay: 0.0009308907319791615\n",
      "\n",
      "Epoch 233/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2055\n",
      "Learning rate with decay: 0.0009306134888902307\n",
      "\n",
      "Epoch 234/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2093\n",
      "Learning rate with decay: 0.0009303364786319435\n",
      "\n",
      "Epoch 235/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2174\n",
      "Learning rate with decay: 0.0009300595847889781\n",
      "\n",
      "Epoch 236/1000\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2071\n",
      "Learning rate with decay: 0.0009297828073613346\n",
      "\n",
      "Epoch 237/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2012\n",
      "Learning rate with decay: 0.0009295063209719956\n",
      "\n",
      "Epoch 238/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2058\n",
      "Learning rate with decay: 0.0009292299509979784\n",
      "\n",
      "Epoch 239/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2051\n",
      "Learning rate with decay: 0.0009289536392316222\n",
      "\n",
      "Epoch 240/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2002\n",
      "Learning rate with decay: 0.0009286776185035706\n",
      "\n",
      "Epoch 241/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2305\n",
      "Learning rate with decay: 0.0009284017723985016\n",
      "\n",
      "Epoch 242/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2364\n",
      "Learning rate with decay: 0.0009281259262934327\n",
      "\n",
      "Epoch 243/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2207\n",
      "Learning rate with decay: 0.0009278504294343293\n",
      "\n",
      "Epoch 244/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2034\n",
      "Learning rate with decay: 0.0009275750489905477\n",
      "\n",
      "Epoch 245/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1999\n",
      "Learning rate with decay: 0.000927299726754427\n",
      "\n",
      "Epoch 246/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2093\n",
      "Learning rate with decay: 0.0009270246955566108\n",
      "\n",
      "Epoch 247/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1950\n",
      "Learning rate with decay: 0.0009267497225664556\n",
      "\n",
      "Epoch 248/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1950\n",
      "Learning rate with decay: 0.000926474982406944\n",
      "\n",
      "Epoch 249/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1992\n",
      "Learning rate with decay: 0.0009262004168704152\n",
      "\n",
      "Epoch 250/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2110\n",
      "Learning rate with decay: 0.0009259259095415473\n",
      "\n",
      "Epoch 251/1000\n",
      " 384/1000 [==========>...................] - ETA: 8s - loss: 0.2063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-073f7b301f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           callbacks=[print_info])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit(images.reshape(-1, 320, 320, 3), labels.reshape(-1, 2), batch_size=10, epochs=10, verbose=1)\n",
    "model.fit(images.reshape(-1, 320, 320, 3),\n",
    "          labels.reshape(-1, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3),\n",
    "          batch_size=BATCHSIZE,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          callbacks=[print_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_IMAGE_INDEX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = model.predict(images[CHECK_IMAGE_INDEX].reshape(1, 320, 320, 3)).reshape(ANCHOR_HEIGHT, ANCHOR_WIDTH, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values found: 1\n",
      "0.8613996\n"
     ]
    }
   ],
   "source": [
    "indicies = np.where(res[:,:,0] == res[:,:,0].max())\n",
    "#print(indicies)\n",
    "print(f\"Max values found: {len(indicies[0])}\")\n",
    "print(res[indicies[0][0], indicies[1][0], 0])\n",
    "#print(res[indicies[0][5], indicies[1][5], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values in labels: 1\n"
     ]
    }
   ],
   "source": [
    "label_indicies = np.where(labels[CHECK_IMAGE_INDEX, :, :, 0] == labels[CHECK_IMAGE_INDEX, :, :, 0].max())\n",
    "print(f\"Max values in labels: {len(label_indicies[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anchors: 400\n",
      "Max value: 0.8613995909690857\n",
      "Number of values above or equal to 0.8613995909690857: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of anchors: {ANCHOR_WIDTH * ANCHOR_HEIGHT}\")\n",
    "max_val = np.max(res[:, :, 0])\n",
    "print(f\"Max value: {max_val}\")\n",
    "above_val = max_val\n",
    "print(f\"Number of values above or equal to {above_val}: {np.count_nonzero(res[:, :, 0] >= above_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max label index: 12\n",
      "Values above threshold: 1\n",
      "Raw offset: (0.34186795353889465, 0.23665359616279602)\n",
      "Predicted point(green) anchor: (167, 198), offset: (-6, -10)\n",
      "\n",
      "max label index: 12\n",
      "Values above threshold: 1\n",
      "Raw offset: (6.0, -7.0)\n",
      "Center(red) point anchor: (152, 198), offset: (6, -7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy the image, so we don't change anything\n",
    "im_res = np.copy(images[CHECK_IMAGE_INDEX])\n",
    "im_res *= 255.0\n",
    "im_res = im_res.astype(np.uint8)\n",
    "\n",
    "for x_res, y_res, x_offset, y_offset in get_all_points_from_prediction(res, threshold=above_val):\n",
    "    prediction_x = x_res + x_offset\n",
    "    prediction_y = y_res + y_offset\n",
    "    cv2.circle(im_res, (prediction_y, prediction_x), 1, (0, 255, 0), thickness=2)\n",
    "    print(f\"Predicted point(green) anchor: ({x_res}, {y_res}), offset: ({x_offset}, {y_offset})\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "x_actual, y_actual, x_act_offset, y_act_offset = get_all_points_from_prediction(labels[CHECK_IMAGE_INDEX],\n",
    "                                                                                do_scale=False)[0]\n",
    "x_label_point = (x_actual + x_act_offset, y_actual + y_act_offset)\n",
    "cv2.circle(im_res, (x_label_point[1], x_label_point[0]), 1, (255, 0, 0), thickness=2)\n",
    "print(f\"Center(red) point anchor: ({x_actual}, {y_actual}), offset: ({x_act_offset}, {y_act_offset})\")\n",
    "\n",
    "f = plt.figure(figsize=(15, 8))\n",
    "plt.imshow(im_res)\n",
    "plt.title(\"Predicted in green, center in red.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_x = res[:, :, 1]\n",
    "res_y = res[:, :, 2]\n",
    "\n",
    "print(f\"Max x prediction: {np.max(res_x)}, min x prediction: {np.min(res_x)}\")\n",
    "print(f\"Max y prediction: {np.max(res_y)}, min y prediction: {np.min(res_y)}\")\n",
    "\n",
    "f, subs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "subs[0].imshow(res_x, cmap='gray')\n",
    "subs[0].set_title(\"X offset predictions\")\n",
    "subs[1].imshow(res_y, cmap='gray')\n",
    "subs[1].set_title(\"Y offset prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "print(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_filterdata(filter_data):\n",
    "    if(np.min(filter_data) < 0):\n",
    "        filter_data = filter_data - np.min(filter_data)\n",
    "\n",
    "    filter_data = (filter_data - np.min(filter_data)) / (np.max(filter_data) - np.min(filter_data))\n",
    "    filter_data_gray = np.zeros((filter_data.shape[0], filter_data.shape[1]))\n",
    "    filter_data_gray = np.mean(filter_data, axis=2)\n",
    "    \n",
    "    return filter_data_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_name = \"conv1\"\n",
    "num_filters = 32\n",
    "\n",
    "col_plots = 4\n",
    "row_plots = int(np.ceil(num_filters / 4))\n",
    "\n",
    "f, subs = plt.subplots(row_plots, col_plots, figsize=(15, 4*row_plots))\n",
    "subs = subs.ravel()\n",
    "\n",
    "layer_output = layer_dict[layer_name].output\n",
    "\n",
    "for i in range(num_filters):\n",
    "    #filter_weights = layer_output[:, :, :, i]\n",
    "    filters = layer_dict[layer_name].get_weights()[0]\n",
    "    filter_index_data = filters[:, :, :, i]\n",
    "    filter_index_gray = get_image_from_filterdata(filter_index_data)\n",
    "\n",
    "    subs[i].imshow(filter_index_gray, cmap='gray')\n",
    "    subs[i].set_title(f\"Filter {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
