{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initially just some playing round with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Image<br>\n",
    "Initial output: center of hand<br>\n",
    "Is anchors needed? So the prediction is an offset?<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SqueezeDetHelpers import fire_layer, binary_crossentropy, keras_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid over image size\n",
    "    - Grid nodes will be anchors\n",
    "    - Net predicts: Probability of class at anchor, and offset from anchor.\n",
    "        - In later versions, several offsets will be predicted at each offset.\n",
    "- The net is fully convolutional, meaning the output must be feature maps.\n",
    "    - Amount of output filters will then be confidence+x_offset+y_offset\n",
    "    - filter size will be the size of the anchor grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_out = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"./data\"\n",
    "ANNOTATION_FILE = r\"annot\"\n",
    "annotation = os.path.join(DATA_DIR, ANNOTATION_FILE)\n",
    "print(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 320\n",
    "WIDTH = 320\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 0 # 0.001\n",
    "KEEP_PROB = 0.5\n",
    "CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_HEIGHT = 20\n",
    "ANCHOR_WIDTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_WEIGHT = 1.0\n",
    "OFFSET_LOSS_WEIGHT = 1.0\n",
    "OFFSET_WEIGHT = 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anchor_nodes = ANCHOR_HEIGHT * ANCHOR_WIDTH\n",
    "\n",
    "print(f\"Out dim: {ANCHOR_HEIGHT}x{ANCHOR_WIDTH}\")\n",
    "print(f\"Number of anchor nodes: {num_anchor_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_anchors():\n",
    "    \n",
    "    #anchors = np.zeros((num_anchor_nodes, 2))\n",
    "    anchors = np.zeros((ANCHOR_HEIGHT, ANCHOR_WIDTH, 2), dtype=np.uint32)\n",
    "    print(f\"Number of anchors: {num_anchor_nodes}\")\n",
    "    \n",
    "    print(f\"Anchor dimension: ({ANCHOR_HEIGHT}, {ANCHOR_WIDTH})\")\n",
    "    print(f\"Anchor shape: {anchors.shape}\")\n",
    "    \n",
    "    #xs = np.arange(PIXELS_BETWEEN_ANCHORS, WIDTH, PIXELS_BETWEEN_ANCHORS)\n",
    "    #ys = np.arange(PIXELS_BETWEEN_ANCHORS, HEIGHT, PIXELS_BETWEEN_ANCHORS)\n",
    "    \n",
    "    x_start = WIDTH / (ANCHOR_WIDTH + 1)\n",
    "    x_end = WIDTH - x_start\n",
    "    y_start = HEIGHT / (ANCHOR_HEIGHT + 1)\n",
    "    y_end = HEIGHT - y_start\n",
    "    xs = np.linspace(x_start, x_end, num=ANCHOR_WIDTH, dtype=np.uint32)\n",
    "    ys = np.linspace(y_start, y_end, num=ANCHOR_HEIGHT, dtype=np.uint32)\n",
    "    \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    for cx in range(len(xs)):\n",
    "        for cy in range(len(ys)):\n",
    "            anchors[counter] = [xs[cx], ys[cy]]\n",
    "            counter += 1\n",
    "    \"\"\"\n",
    "    \n",
    "    for ix in range(ANCHOR_HEIGHT):\n",
    "        for iy in range(ANCHOR_WIDTH):\n",
    "            anchors[ix, iy] = (xs[ix], ys[iy])\n",
    "    \n",
    "    return anchors\n",
    "    \n",
    "anchs = set_anchors()\n",
    "anchs[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(HEIGHT, WIDTH, CHANNELS), name=\"input\")\n",
    "print(f\"input: {input_layer.shape}\")\n",
    "\n",
    "conv1 = Conv2D(name='conv1', filters=128, kernel_size=(3, 3), strides=(2, 2), activation=None, padding=\"SAME\",\n",
    "               use_bias=False,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.01),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(input_layer)\n",
    "print(f\"conv1: {conv1.shape}\")\n",
    "\n",
    "bn = BatchNormalization(name='bn')(conv1)\n",
    "act = Activation('relu', name='act')(bn)\n",
    "\n",
    "pool1 = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='SAME', name=\"pool1\")(act)\n",
    "print(f\"pool1: {pool1.shape}\")\n",
    "\n",
    "fire1_1 = fire_layer(name=\"fire1_1\", input=pool1, s1x1=32, e1x1=128, e3x3=128, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire1_1: {fire1_1.shape}\")\n",
    "\n",
    "fire1_2 = fire_layer(name=\"fire1_2\", input=fire1_1, s1x1=32, e1x1=128, e3x3=128, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire1_2: {fire1_2.shape}\")\n",
    "\n",
    "#fire1_3 = fire_layer(name=\"fire1_3\", input=fire1_2, s1x1=32, e1x1=128, e3x3=128, weight_decay=WEIGHT_DECAY)\n",
    "#print(f\"fire1_3: {fire1_3.shape}\")\n",
    "\n",
    "pool2 = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='SAME', name=\"pool2\")(fire1_2)\n",
    "print(f\"pool2: {pool2.shape}\")\n",
    "\n",
    "fire2_1 = fire_layer(name=\"fire2_1\", input=pool2, s1x1=48, e1x1=192, e3x3=192, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire2_1: {fire2_1.shape}\")\n",
    "\n",
    "fire2_2 = fire_layer(name=\"fire2_2\", input=fire2_1, s1x1=48, e1x1=192, e3x3=192, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire2_2: {fire2_2.shape}\")\n",
    "\n",
    "#fire2_3 = fire_layer(name=\"fire2_3\", input=fire2_2, s1x1=48, e1x1=192, e3x3=192, weight_decay=WEIGHT_DECAY)\n",
    "#print(f\"fire2_3: {fire2_3.shape}\")\n",
    "\n",
    "pool3 = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='SAME', name=\"pool3\")(fire2_2)\n",
    "print(f\"pool3: {pool3.shape}\")\n",
    "\n",
    "fire3_1 = fire_layer(name=\"fire3_1\", input=pool3, s1x1=48, e1x1=192, e3x3=192, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire3_1: {fire3_1.shape}\")\n",
    "\n",
    "fire3_2 = fire_layer(name=\"fire3_2\", input=fire3_1, s1x1=48, e1x1=192, e3x3=192, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire3_2: {fire3_2.shape}\")\n",
    "\n",
    "\"\"\"\n",
    "fire4 = fire_layer(name=\"fire4\", input=pool2, s1x1=16, e1x1=64, e3x3=64, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire4: {fire4.shape}\")\n",
    "\n",
    "fire5 = fire_layer(name=\"fire5\", input=fire4, s1x1=16, e1x1=64, e3x3=64, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire5: {fire5.shape}\")\n",
    "\n",
    "fire6 = fire_layer(name=\"fire6\", input=fire5, s1x1=16, e1x1=64, e3x3=64, weight_decay=WEIGHT_DECAY)\n",
    "print(f\"fire6: {fire6.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "conv2 = Conv2D(name='conv2', filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"SAME\",\n",
    "               #filters=len(ANCHORS),\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(pool2)\n",
    "print(f\"conv2: {conv2.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "conv3 = Conv2D(name='conv3', filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"SAME\",\n",
    "               #filters=len(ANCHORS),\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(conv2)\n",
    "print(f\"conv3: {conv3.shape}\")\n",
    "\n",
    "conv4 = Conv2D(name='conv4', filters=256, kernel_size=(3, 3), strides=(2, 2), activation='relu', padding=\"SAME\",\n",
    "               #filters=len(ANCHORS),\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "               )(conv3)\n",
    "print(f\"conv4: {conv4.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "preds = Conv2D(name='preds', filters=num_out, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid', padding=\"SAME\",\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.001),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY)\n",
    "               )(fire6)\n",
    "print(f\"preds: {preds.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "#drop1 = Dropout(rate=KEEP_PROB, name=\"drop1\")(fire6)\n",
    "\n",
    "pred_conf = Conv2D(name='pred_conf', filters=1, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid', padding=\"SAME\",\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.01),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY)\n",
    "               )(fire3_2)\n",
    "print(f\"pred_conf: {pred_conf.shape}\")\n",
    "\n",
    "pred_offset = Conv2D(name='pred_offset', filters=2, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid', padding=\"SAME\",\n",
    "               #use_bias=True,\n",
    "               kernel_initializer=TruncatedNormal(stddev=0.01),\n",
    "               #kernel_regularizer=l2(WEIGHT_DECAY)\n",
    "               )(fire3_2)\n",
    "print(f\"pred_offset: {pred_offset.shape}\")\n",
    "\n",
    "#preds = Concatenate()([pred_conf, pred_offset])\n",
    "preds = concatenate([pred_conf, pred_offset])\n",
    "print(f\"preds: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-entropy: q * -log(p) + (1-q) * -log(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    # We are predicting a batchsize x anchorwidth x anchorheight x 3 output.\n",
    "    c_predictions = y_pred[:, :, :, 0]\n",
    "    c_labels = y_true[:, :, :, 0]\n",
    "    \n",
    "    pred_offset_x = 2 * (y_pred[:, :, :, 1] - 0.5) * OFFSET_WEIGHT\n",
    "    pred_offset_y = 2 * (y_pred[:, :, :, 2] - 0.5) * OFFSET_WEIGHT\n",
    "    \n",
    "    true_offset_x = y_true[:, :, :, 1]\n",
    "    true_offset_y = y_true[:, :, :, 2]\n",
    "    \n",
    "    #offset_mask = K.copy(true_offset[:,:,:,0])\n",
    "    #offset_mask = K.abs(true_offset[:,:,:,0]) + K.abs(true_offset[:,:,:,0])\n",
    "    \n",
    "    #offset_mask[offset_mask!=0] = 1.0\n",
    "    \n",
    "    #offset_mask = K.zeros((BATCHSIZE, ANCHOR_WIDTH, ANCHOR_HEIGHT))\n",
    "    #m = K.where((true_offset[:,:,:,0]!=0) | (true_offset[:,:,:,1]!=0))\n",
    "    #offset_mask[true_offset[:,:,:,0]!=0] = 1.0\n",
    "    #offset_mask[true_offset[:,:,:,1]!=0] = 1.0\n",
    "    \n",
    "    g_x = K.less(true_offset_x, 0)\n",
    "    l_x = K.greater(true_offset_x, 0)\n",
    "    g_y = K.greater(true_offset_y, 0)\n",
    "    l_y = K.less(true_offset_y, 0)\n",
    "    \n",
    "    g_x_i = K.cast(g_x, dtype='float32')\n",
    "    l_x_i = K.cast(l_x, dtype='float32')\n",
    "    g_y_i = K.cast(g_y, dtype='float32')\n",
    "    l_y_i = K.cast(l_y, dtype='float32')\n",
    "\n",
    "    mask_offset_x = K.clip(g_x_i + l_x_i, 0, 1.0)\n",
    "    mask_offset_y = K.clip(g_y_i + l_y_i, 0, 1.0)\n",
    "    #zeroes = K.zeros_like(true_offset_x)\n",
    "    #ones = K.ones_like(true_offset_x)\n",
    "    #mask_offset = K.where(mask_offset_bool, ones, zeroes)\n",
    "    \n",
    "    #pred_conf = K.sigmoid(c_predictions)\n",
    "    #pred_conf = c_predictions\n",
    "\n",
    "    #c_loss = K.sum(\n",
    "    #    -(c_labels * K.log(pred_conf + EPSILON) + (1-c_labels) * K.log(1-pred_conf + EPSILON))\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    c_labels * (-K.log(c_predictions + EPSILON)) + (1-c_labels) * (-K.log(1-c_predictions + EPSILON)) * c_labels\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.maximum(K.abs(c_predictions), 0) - c_predictions * c_labels + K.log(1 + K.exp(-K.abs(c_predictions)))\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.maximum(K.abs(c_predictions), 0) - c_predictions * c_labels + K.log(1 + K.exp(-K.abs(c_predictions)))\n",
    "    #, axis=0) / BATCHSIZE\n",
    "\n",
    "    #c_loss = K.sum(c_loss) / (ANCHOR_HEIGHT * ANCHOR_WIDTH)\n",
    "    \n",
    "    #diff = K.abs(c_labels - c_predictions)\n",
    "    #c_loss = 2 * (K.sigmoid(diff) - 0.5)\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.sigmoid(\n",
    "    #        K.abs(\n",
    "    #            c_labels - c_predictions\n",
    "    #        )\n",
    "    #    )\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    2 * K.sigmoid(\n",
    "    #        K.abs(\n",
    "    #            c_labels - c_predictions\n",
    "    #        )\n",
    "    #    ) - 0.5\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #c_loss = K.sum(\n",
    "    #    K.abs(\n",
    "    #        c_labels - c_predictions\n",
    "    #    )\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    #c_loss = K.sig\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = K.sum(c_labels)  # Accounts for batch size\n",
    "    num_non_labels = ANCHOR_WIDTH * ANCHOR_HEIGHT - num_labels\n",
    "    \n",
    "    # Loss matrix for all entries\n",
    "    loss_m_all = keras_binary_crossentropy(c_labels, c_predictions, EPSILON)\n",
    "    \n",
    "    # Loss matrix for the correct label\n",
    "    loss_m_label = keras_binary_crossentropy(c_labels, c_predictions, EPSILON) * c_labels\n",
    "    \n",
    "    # Loss matrix for non labels\n",
    "    loss_m_nonlabel = loss_m_all - loss_m_label\n",
    "    \n",
    "    # Summing and adding weight to label loss\n",
    "    c_loss_label = K.sum(\n",
    "        loss_m_label\n",
    "    ) / num_labels\n",
    "    \n",
    "    # summing and adding weight to non label loss\n",
    "    c_loss_nonlabel = K.sum(\n",
    "        loss_m_nonlabel\n",
    "    ) / num_non_labels\n",
    "    \n",
    "    c_loss = c_loss_label * LABEL_WEIGHT + c_loss_nonlabel * (1 / LABEL_WEIGHT)\n",
    "    \n",
    "    \n",
    "    o_loss_x = K.sum(\n",
    "        #K.pow(true_offset_x - pred_offset_x, 2) * mask_offset_x\n",
    "        #K.abs(true_offset_x - pred_offset_x) * mask_offset_x\n",
    "        K.square((true_offset_x - pred_offset_x) * mask_offset_x)\n",
    "    ) / num_labels\n",
    "    \n",
    "    o_loss_y = K.sum(\n",
    "        #K.pow(true_offset_y - pred_offset_y, 2) * mask_offset_y\n",
    "        #K.abs(true_offset_y - pred_offset_y) * mask_offset_y\n",
    "        K.square((true_offset_y - pred_offset_y) * mask_offset_y)\n",
    "    ) / num_labels\n",
    "    \n",
    "    o_loss = (o_loss_x + o_loss_y) * OFFSET_LOSS_WEIGHT\n",
    "    \n",
    "    # Only on correct label\n",
    "    #o_loss = K.sum(\n",
    "    #    #K.pow(pred_offset - true_offset, 2) * offset_mask\n",
    "    #    (K.pow(\n",
    "    #        true_offset[:, :, :, 0] - pred_offset[:, :, :, 0], 2\n",
    "    #    ) + K.pow(\n",
    "    #        true_offset[:, :, :, 1] - pred_offset[:, :, :, 1], 2\n",
    "    #    ))# * offset_mask\n",
    "    #) / BATCHSIZE / num_labels\n",
    "    \n",
    "    #l2_loss = K.sum(\n",
    "    #    2 * K.sigmoid (\n",
    "    #        K.sqrt(\n",
    "    #            K.pow(y_true_offset[0] - y_true_offset[0], 2) + K.pow(y_true_offset[1] - y_pred_offset[1], 2)\n",
    "    #        )\n",
    "    #    ) - 0.5\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    #l2_loss = K.sum(\n",
    "    #    2 * K.sigmoid(\n",
    "    #        K.pow(y_pred_offset - y_true_offset, 2)\n",
    "    #    ) - 0.5\n",
    "    #) / BATCHSIZE\n",
    "    \n",
    "    \n",
    "    #o_loss = 0\n",
    "    \n",
    "    total_loss = o_loss + c_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "#y_true_test = np.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "#y_pred_test = np.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "#l = loss(y_pred_test, y_true_test)\n",
    "#print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "x = np.linspace(-10, 10, 500)\n",
    "y = 2 * (sigmoid(x) - 0.5)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$−(ylog(p)+(1−y)log(1−p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/48951109/keras-custom-binary-cross-entropy-loss-function-get-nan-as-output-for-loss<br>\n",
    "$max(x, 0) - x * z + log(1 + exp(-abs(x)))$<br>\n",
    "$max(p, 0) - p * y + log(1 + exp(-abs(p)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_labels = 1\n",
    "pred_conf = 0\n",
    "\n",
    "diff = abs(c_labels - pred_conf)\n",
    "l = (sigmoid(diff) - 0.5) * 2\n",
    "#print(l)\n",
    "\n",
    "#pred_conf = sigmoid(pred_conf)\n",
    "#ll = -(c_labels * np.log(pred_conf + EPSILON) + (1 - c_labels) * np.log(1-pred_conf + EPSILON))\n",
    "#print(ll)\n",
    "\n",
    "lll = np.max(np.abs(pred_conf), 0) - pred_conf * c_labels + np.log(1 + np.exp(-np.abs(pred_conf)))\n",
    "print(lll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = np.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "y_t[:, 10, 10, 0] = 1.0\n",
    "print(y_t.shape)\n",
    "#np.max(np.maximum(-y_t, 0))\n",
    "np.sum(y_t) / BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([[1, 2, 3],\n",
    "                 [1, 2, 3],\n",
    "                 [1, 2, 3]])\n",
    "\n",
    "arr2 = np.array([[4, 5, 6],\n",
    "                 [4, 5, 6],\n",
    "                 [4, 5, 6]])\n",
    "\n",
    "res = np.concatenate((arr1, arr2), axis=-1)\n",
    "print(arr1.shape)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_loss(y_true, y_pred, bs):\n",
    "    #c_predictions = y_pred[:, :, :, 0]\n",
    "    #c_labels = y_true[:, :, :, 0]\n",
    "\n",
    "    c_predictions = y_pred\n",
    "    c_labels = y_true\n",
    "\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = np.sum(c_labels)\n",
    "    num_non_labels = 3 * 3 - num_labels   # ANCHOR_WIDTH and ANCHOR_HEIGHT\n",
    "    \n",
    "    # Loss matrix for all entries\n",
    "    loss_m_all = binary_crossentropy(c_labels, c_predictions, EPSILON)\n",
    "    \n",
    "    # Loss matrix for the correct label\n",
    "    loss_m_label = binary_crossentropy(c_labels, c_predictions, EPSILON) * c_labels\n",
    "    #print(loss_m_label)\n",
    "    \n",
    "    # Loss matrix for non labels\n",
    "    loss_m_nonlabel = loss_m_all - loss_m_label\n",
    "    #print(loss_m_nonlabel)\n",
    "    \n",
    "    # Summing and adding weight to label loss\n",
    "    c_loss_label = np.sum(\n",
    "        loss_m_label\n",
    "    ) / bs / num_labels\n",
    "    \n",
    "    # summing and adding weight to non label loss\n",
    "    c_loss_nonlabel = np.sum(\n",
    "        loss_m_nonlabel\n",
    "    ) / bs / num_non_labels\n",
    "    \n",
    "    print(f\"Label loss: {c_loss_label}\")\n",
    "    print(f\"Non label loss: {c_loss_nonlabel}\")\n",
    "    \n",
    "    c_loss = c_loss_label * LABEL_WEIGHT + c_loss_nonlabel * (1 / LABEL_WEIGHT)\n",
    "    \n",
    "    return c_loss\n",
    "\n",
    "\n",
    "#labels_test = np.copy(labels[-20:])\n",
    "#print(labels_test[:, :, :, 0].shape)\n",
    "#labels_test[:, :, :, 0] = 2\n",
    "#np_loss(labels[:20], labels_test)\n",
    "#print(np_loss(labels[-5:], labels[:5], 5))\n",
    "#print(np_loss(labels[:20], labels[:20], 20))\n",
    "\n",
    "\n",
    "true_test = np.array([[0, 0, 0],\n",
    "                      [0, 1, 0],\n",
    "                      [0, 0, 0]])\n",
    "\n",
    "pred_test = np.array([[0, 1, 0],\n",
    "                      [0, 1, 0],\n",
    "                      [0, 0, 0]])\n",
    "\n",
    "np_loss(true_test, pred_test, bs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "\n",
    "\"\"\"\n",
    "with tf.Session() as s:\n",
    "    # Some tensor we want to print the value of\n",
    "    y_true_test = tf.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    y_pred_test = tf.zeros((BATCHSIZE, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    \n",
    "    l = loss(y_pred_test, y_true_test)\n",
    "    # Add print operation\n",
    "    print(s.run(l))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_labels = 0\n",
    "c_predictions = 1\n",
    "c_loss = (c_labels * (-np.log(c_predictions + EPSILON))) + (1-c_labels) * (-np.log(1-c_predictions + EPSILON))\n",
    "print(c_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(inputs=input_layer, outputs=preds)\n",
    "#model.compile(loss='mse', optimizer='adam')\n",
    "#model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    with open(annotation, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    gt = [(None, None)] * len(lines)\n",
    "    gt = np.zeros((len(lines), 2))\n",
    "    \n",
    "    for l in lines:\n",
    "        obj = l.split(',')\n",
    "        pic_id = int(obj[0].split('.')[0])\n",
    "        x = int(obj[1])\n",
    "        y = int(obj[2])\n",
    "        \n",
    "        gt[pic_id] = (x, y)\n",
    "\n",
    "    images = []\n",
    "    \n",
    "    for fi in os.listdir(DATA_DIR):\n",
    "        if not fi.endswith('jpg'):\n",
    "            continue\n",
    "        im = cv2.imread(os.path.join(DATA_DIR, fi))\n",
    "        images.append(im)\n",
    "    \n",
    "    return gt, images\n",
    "\n",
    "labels_clean, images_clean = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def closest_anchor_map(x, y, anchor_coords):\n",
    "    \"\"\" Create a anchor_height x anchor_width x 3 map.\n",
    "        First entry is 1 if the anchor point is closest to true point. Zero otherwise.\n",
    "        Second is x offset.\n",
    "        Third is y offset. \"\"\"\n",
    "    closest = 10000\n",
    "    closest_x = None\n",
    "    closest_y = None\n",
    "    closest_x_offset = None\n",
    "    closest_y_offset = None\n",
    "    \n",
    "    res = np.zeros((ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    for ix in range(ANCHOR_HEIGHT):\n",
    "        for iy in range(ANCHOR_WIDTH):\n",
    "            p_x, p_y = anchor_coords[ix, iy]\n",
    "            dist = np.sqrt( (x - p_x)**2 + (y - p_y)**2 )\n",
    "            #res[ix, iy, 1:] = (x - p_x, y - p_y)\n",
    "            if dist < closest:\n",
    "                closest = dist\n",
    "                closest_x = ix\n",
    "                closest_y = iy\n",
    "                closest_x_offset = (x - p_x)\n",
    "                closest_y_offset = (y - p_y)\n",
    "    \n",
    "    #print(f\"({closest_x}, {closest_y}) -> {anchor_coords[closest_x, closest_y]}\")\n",
    "    res[closest_x, closest_y, 0] = 1\n",
    "    res[closest_x, closest_y, 1:] = (closest_x_offset, closest_y_offset)\n",
    "    \n",
    "    return res\n",
    "        \n",
    "test_map = closest_anchor_map(20, 30, anchs)\n",
    "print(test_map.shape)\n",
    "print(np.count_nonzero(test_map[:,:, 0]))\n",
    "print(np.mean(test_map[:, :, 1]))\n",
    "print(np.mean(test_map[:, :, 2]))\n",
    "anc_indicies = np.where(test_map[:, :, 0] == test_map[:, :, 0].max())\n",
    "print(test_map[anc_indicies[0], anc_indicies[1]])\n",
    "anchor_point = test_map[anc_indicies[0], anc_indicies[1]][:,1:][0]\n",
    "print(anchs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_anchors():\n",
    "    # load images\n",
    "    # labels will be:\n",
    "    #   anchor_height x anchor_width x 3\n",
    "    #     the last 3 entries is: 1 if closest gridpoint to a point. x and y offsets to closest point.\n",
    "    with open(annotation, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    gt = np.zeros((len(lines), ANCHOR_HEIGHT, ANCHOR_WIDTH, 3))\n",
    "    gt_clean = [(None, None)] * len(lines)\n",
    "    images = np.zeros((len(lines), HEIGHT, WIDTH, 3))#, dtype=np.uint8)\n",
    "    \n",
    "    for c, l in enumerate(tqdm(lines)):\n",
    "        obj = l.split(',')\n",
    "        pic_id = int(obj[0].split('.')[0])\n",
    "        x = int(obj[1])\n",
    "        y = int(obj[2])\n",
    "        \n",
    "        gt[pic_id, :, :] = closest_anchor_map(x, y, anchs)\n",
    "        gt_clean[pic_id] = (x, y)\n",
    "    \n",
    "    #images = []\n",
    "    \n",
    "    for fi in tqdm(os.listdir(DATA_DIR)):\n",
    "        if not fi.endswith('jpg'):\n",
    "            continue\n",
    "        im = cv2.imread(os.path.join(DATA_DIR, fi))\n",
    "        #images.append(im)\n",
    "        i = int(fi.split('.')[0])\n",
    "        images[i] = im / 255.0\n",
    "    \n",
    "    images = np.array(images)\n",
    "    \n",
    "    return gt, gt_clean, images\n",
    "        \n",
    "labels, labels_clean, images = load_data_with_anchors()\n",
    "print(labels.shape)\n",
    "print(images[0].dtype)\n",
    "\n",
    "num_data = len(labels)\n",
    "num_validation_data = int(VALIDATION_SPLIT * num_data)\n",
    "validation_labels = labels[:num_validation_data]\n",
    "validation_images = images[:num_validation_data]\n",
    "labels = labels[num_validation_data:]\n",
    "images = images[num_validation_data:]\n",
    "\n",
    "print(len(validation_labels))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(labels[5][0][0])\n",
    "#np.max(images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it everythin gets loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_points_from_prediction(pred, threshold=1.0, do_scale=True):\n",
    "    \"\"\"\n",
    "    pred is a prediction map in the shape (ANCHOR_HEIGHT, ANCHOR_WIDTH, 3)\n",
    "    \"\"\"\n",
    "    # Get all points with a confidence above threshold\n",
    "    label_indicies = np.where(pred[:, :, 0] >= threshold)\n",
    "    num_points = len(label_indicies[0])\n",
    "    #print(f\"max label index: {np.max(label_indicies)}\")\n",
    "    #print(f\"Values above threshold: {num_points}\")\n",
    "    \n",
    "    points = np.zeros((num_points, 4))#, dtype=np.int32)\n",
    "    \n",
    "    # Loop through all anchor points\n",
    "    for c, (x_anchor, y_anchor) in enumerate(zip(label_indicies[0], label_indicies[1])):\n",
    "        #print(x_anchor)\n",
    "        # when anchor location is known, the location of the closest anchor in the actual image can be found\n",
    "        x_without_offset, y_without_offset = anchs[x_anchor, y_anchor]\n",
    "        #print(f\"Anchor: ({x_anchor, y_anchor})\")\n",
    "        \n",
    "        # The offset can then be extracted from the labels\n",
    "        (x_offset, y_offset) = pred[label_indicies[0], label_indicies[1]][0][1:]\n",
    "        #print(f\"Raw offset: ({x_offset}, {y_offset})\")\n",
    "        if do_scale:\n",
    "            x_offset = 2 * (x_offset - 0.5) * OFFSET_WEIGHT\n",
    "            y_offset = 2 * (y_offset - 0.5) * OFFSET_WEIGHT\n",
    "        #print(f\"({x_offset}, {y_offset})\")\n",
    "        \n",
    "        # and the final point calculated\n",
    "        #actual_x = int(x_without_offset + x_offset)\n",
    "        #actual_y= int(y_without_offset + y_offset)\n",
    "\n",
    "        points[c] = (x_without_offset, y_without_offset, x_offset, y_offset)\n",
    "    \n",
    "    return points.astype(np.int32)\n",
    "    \n",
    "\n",
    "p = labels[0]\n",
    "#print(p.shape)\n",
    "pp = get_all_points_from_prediction(p)\n",
    "print(pp)\n",
    "print(\"\")\n",
    "pp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "im = np.copy(images[index]) * 255.0\n",
    "#im = im.astype(np.uint8)\n",
    "\n",
    "im_anchors = np.copy(images[index]) * 255.0\n",
    "#im = im.astype(np.uint8)\n",
    "\n",
    "print(anchs[1, 1])\n",
    "\n",
    "# draw all anchors\n",
    "for anc_x in range(ANCHOR_HEIGHT):\n",
    "    for anc_y in range(ANCHOR_WIDTH):\n",
    "        cv2.circle(im_anchors, (anchs[anc_x, anc_y][0], anchs[anc_x, anc_y][1]), 1, (128, 128, 128), thickness=1)\n",
    "\n",
    "\n",
    "\n",
    "# Get labels with max value, we know this to only be one in the labels\n",
    "label_indicies = np.where(labels[index, :, :, 0] == labels[index, :, :, 0].max())\n",
    "print(f\"Max values in labels: {len(label_indicies[0])}\")\n",
    "\n",
    "# Get location in the anchor\n",
    "#x_anchor, y_anchor = label_indicies\n",
    "# when anchor location is known, the location of the closest anchor in the actual image can be found\n",
    "#x_without_offset, y_without_offset = anchs[x_anchor[0], y_anchor[0]]\n",
    "\"\"\"\n",
    "# The offset can then be extracted from the labels\n",
    "(x_offset, y_offset) = labels[index, label_indicies[0], label_indicies[1]][0][1:]\n",
    "# and the final point calculated\n",
    "actual_x = int(x_without_offset + x_offset)\n",
    "actual_y= int(y_without_offset + y_offset)\n",
    "print(f\"Actual point: ({actual_x}, {actual_y})\")\n",
    "\"\"\"\n",
    "actual_x_anch, actual_y_anch, actual_x_off, actual_y_off = get_all_points_from_prediction(labels[index],\n",
    "                                                                                          do_scale=False)[0]\n",
    "actual_point = (actual_x_anch + actual_x_off, actual_y_anch + actual_y_off)\n",
    "\n",
    "cv2.circle(im, (actual_y_anch, actual_x_anch), 2, (0, 255, 0), thickness=2)\n",
    "cv2.circle(im, (actual_point[1], actual_point[0]), 2, (255, 0, 0), thickness=2)\n",
    "print(f\"Anchor: ({actual_x_anch}, {actual_y_anch})\")\n",
    "print(labels[index, label_indicies[0], label_indicies[1]])\n",
    "\n",
    "f, subs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "subs[0].imshow(im_anchors.astype(np.uint8))\n",
    "subs[0].set_title(\"Anchor points\")\n",
    "subs[1].imshow(im.astype(np.uint8))\n",
    "subs[1].set_title(\"Red is center, green is closest anchor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.array(labels)#.reshape(1, 100, 2)\n",
    "#print(labels.shape)\n",
    "#print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c, i in enumerate(images):\n",
    "#    model.fit(i.reshape(1, 320, 320, 3), labels[c].reshape(1, 2), epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(images.reshape(-1, 320, 320, 3), labels.reshape(-1, 2), batch_size=10, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def loss(y_true, y_pred):\n",
    "#    return K.sqrt(K.sum(K.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintInfo(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print(f\"Learning rate with decay: {K.eval(lr_with_decay)}\")\n",
    "        #print(f\"lr={K.eval(lr)}, decay={K.eval(decay)}\")\n",
    "        print(\"\")\n",
    "        \n",
    "print_info = PrintInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=1e-4, decay=1e-4) #, clipnorm=1.0)\n",
    "#opt = optimizers.RMSprop(lr=0.001)#,  clipnorm=1.0)\n",
    "#opt = optimizers.SGD(lr=0.01, decay=0.001, momentum=0.9, nesterov=False)\n",
    "#opt = optimizers.Adagrad(lr=1e-3, decay=1e-3, clipnorm=1.0)\n",
    "#opt =optimizers.SGD()\n",
    "model.compile(loss=loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(images.reshape(-1, 320, 320, 3), labels.reshape(-1, 2), batch_size=10, epochs=10, verbose=1)\n",
    "model.fit(images.reshape(-1, 320, 320, 3),\n",
    "          labels.reshape(-1, ANCHOR_HEIGHT, ANCHOR_WIDTH, 3),\n",
    "          batch_size=BATCHSIZE,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          callbacks=[print_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('model.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_IMAGE_INDEX = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "before = time.time()\n",
    "res = model.predict(images[CHECK_IMAGE_INDEX].reshape(1, 320, 320, 3)).reshape(ANCHOR_HEIGHT, ANCHOR_WIDTH, 3)\n",
    "after = time.time()\n",
    "print(f\"Forward pass time: {after-before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indicies = np.where(res[:,:,0] == res[:,:,0].max())\n",
    "#print(indicies)\n",
    "print(f\"Max values found: {len(indicies[0])}\")\n",
    "print(res[indicies[0][0], indicies[1][0], 0])\n",
    "#print(res[indicies[0][5], indicies[1][5], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indicies = np.where(labels[CHECK_IMAGE_INDEX, :, :, 0] == labels[CHECK_IMAGE_INDEX, :, :, 0].max())\n",
    "print(f\"Max values in labels: {len(label_indicies[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of anchors: {ANCHOR_WIDTH * ANCHOR_HEIGHT}\")\n",
    "max_val = np.max(res[:, :, 0])\n",
    "print(f\"Max value: {max_val}\")\n",
    "above_val = max_val\n",
    "print(f\"Number of values above or equal to {above_val}: {np.count_nonzero(res[:, :, 0] >= above_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Copy the image, so we don't change anything\n",
    "im_res = np.copy(images[CHECK_IMAGE_INDEX])\n",
    "im_res *= 255.0\n",
    "im_res = im_res.astype(np.uint8)\n",
    "\n",
    "for x_res, y_res, x_offset, y_offset in get_all_points_from_prediction(res, threshold=above_val):\n",
    "    prediction_x = x_res + x_offset\n",
    "    prediction_y = y_res + y_offset\n",
    "    cv2.circle(im_res, (prediction_y, prediction_x), 1, (0, 255, 0), thickness=3)\n",
    "    print(f\"Predicted point(green) anchor: ({x_res}, {y_res}), offset: ({x_offset}, {y_offset})\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "x_actual, y_actual, x_act_offset, y_act_offset = get_all_points_from_prediction(labels[CHECK_IMAGE_INDEX],\n",
    "                                                                                do_scale=False)[0]\n",
    "x_label_point = (x_actual + x_act_offset, y_actual + y_act_offset)\n",
    "cv2.circle(im_res, (x_label_point[1], x_label_point[0]), 1, (255, 0, 0), thickness=2)\n",
    "print(f\"Center(red) point anchor: ({x_actual}, {y_actual}), offset: ({x_act_offset}, {y_act_offset})\")\n",
    "\n",
    "f = plt.figure(figsize=(15, 8))\n",
    "plt.imshow(im_res)\n",
    "plt.title(\"Predicted in green, center in red.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_x = res[:, :, 1]\n",
    "res_y = res[:, :, 2]\n",
    "\n",
    "print(f\"Max x prediction: {np.max(res_x)}, min x prediction: {np.min(res_x)}\")\n",
    "print(f\"Max y prediction: {np.max(res_y)}, min y prediction: {np.min(res_y)}\")\n",
    "\n",
    "f, subs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "subs[0].imshow(res_x, cmap='gray')\n",
    "subs[0].set_title(\"X offset predictions\")\n",
    "subs[1].imshow(res_y, cmap='gray')\n",
    "subs[1].set_title(\"Y offset prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "print(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_filterdata(filter_data):\n",
    "    if(np.min(filter_data) < 0):\n",
    "        filter_data = filter_data - np.min(filter_data)\n",
    "\n",
    "    filter_data = (filter_data - np.min(filter_data)) / (np.max(filter_data) - np.min(filter_data))\n",
    "    filter_data_gray = np.zeros((filter_data.shape[0], filter_data.shape[1]))\n",
    "    filter_data_gray = np.mean(filter_data, axis=2)\n",
    "    \n",
    "    return filter_data_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_name = \"conv1\"\n",
    "num_filters = 32\n",
    "\n",
    "col_plots = 4\n",
    "row_plots = int(np.ceil(num_filters / 4))\n",
    "\n",
    "f, subs = plt.subplots(row_plots, col_plots, figsize=(15, 4*row_plots))\n",
    "subs = subs.ravel()\n",
    "\n",
    "layer_output = layer_dict[layer_name].output\n",
    "\n",
    "for i in range(num_filters):\n",
    "    #filter_weights = layer_output[:, :, :, i]\n",
    "    filters = layer_dict[layer_name].get_weights()[0]\n",
    "    filter_index_data = filters[:, :, :, i]\n",
    "    filter_index_gray = get_image_from_filterdata(filter_index_data)\n",
    "\n",
    "    subs[i].imshow(filter_index_gray, cmap='gray')\n",
    "    subs[i].set_title(f\"Filter {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data, labels):\n",
    "    \n",
    "    error = 0\n",
    "    min_error = None\n",
    "    max_error = None\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        p = model.predict(data[i].reshape(1, 320, 320, 3)).reshape(ANCHOR_HEIGHT, ANCHOR_WIDTH, 3)\n",
    "        \n",
    "        # Note, only using first point\n",
    "        predicted_points = get_all_points_from_prediction(p, threshold=np.max(p[:, :, 0]))[0]\n",
    "        predicted_x = predicted_points[0] + predicted_points[2]\n",
    "        predicted_y = predicted_points[1] + predicted_points[3]\n",
    "\n",
    "        label_point = get_all_points_from_prediction(labels[i], do_scale=False)[0]\n",
    "        label_x = label_point[0] + label_point[2]\n",
    "        label_y = label_point[1] + label_point[3]\n",
    "        \n",
    "        dist = np.sqrt((predicted_x - label_x)**2 + (predicted_y - label_y)**2)\n",
    "        #print(f\"{i}: {dist}, ({label_x}, {label_y}) -> ({predicted_x}, {predicted_y})\")\n",
    "        \n",
    "        if min_error is None:\n",
    "            min_error = dist\n",
    "        else:\n",
    "            if dist < min_error:\n",
    "                min_error = dist\n",
    "                \n",
    "        if max_error is None:\n",
    "            max_error = dist\n",
    "        else:\n",
    "            if dist > max_error:\n",
    "                max_error = dist\n",
    "        \n",
    "        error += dist\n",
    "\n",
    "    error /= (i+1)\n",
    "    \n",
    "    return error, min_error, max_error\n",
    "    \n",
    "e, min_e, max_e = accuracy(validation_images, validation_labels)\n",
    "print(f\"Average error: {e}\")\n",
    "print(f\"min error: {min_e}\")\n",
    "print(f\"max error: {max_e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
